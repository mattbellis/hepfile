{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a158213-b057-487f-abf8-f92bf7623f8a",
   "metadata": {},
   "source": [
    "# CSVS to Hepfile\n",
    "\n",
    "Imagine we have a database like structure with multiple csvs all connected by a common ID. See the household exmaple here: https://hepfile.readthedocs.io/en/latest/fundamentals.html#a-toy-example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2b0b71-fbfe-422c-b2a0-c9b9d84a248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import hepfile as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee4d028-519a-4429-87dc-fde0a2d5e102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Residences': {'Household ID': <Array [[0], [1], [2], [3]] type='4 * var * int64'>,\n",
       "  'House/apartment/condo': <Array [['House'], ['Apartment'], ..., ['House']] type='4 * var * string'>,\n",
       "  '# of bedrooms': <Array [[4], [2], [2], [6]] type='4 * var * int64'>,\n",
       "  '# of bathrooms': <Array [[2.5], [2], [1], [4.5]] type='4 * var * float64'>,\n",
       "  'Square footage': <Array [[1500], [1200], [1000], [4500]] type='4 * var * int64'>,\n",
       "  'Year built': <Array [[1955], [2002], [2014], [1998]] type='4 * var * int64'>,\n",
       "  'Estimate': <Array [[250000], [1400], [325000], [500000]] type='4 * var * int64'>},\n",
       " 'People': {'Household ID': <Array [[0, 0, 0, 0], [...], ..., [3, 3, 3, 3, 3, 3, 3]] type='4 * var * int64'>,\n",
       "  'First name': <Array [['blah', 'blah', 'blah', 'blah'], ..., [...]] type='4 * var * string'>,\n",
       "  'Last name': <Array [['blah', 'blah', 'blah', 'blah'], ..., [...]] type='4 * var * string'>,\n",
       "  'Gender ID': <Array [['M', 'F', 'NB', 'F'], ..., ['M', ..., 'F']] type='4 * var * string'>,\n",
       "  'Age': <Array [[54, 52, 18, 14], ..., [54, 52, ..., 11, 65]] type='4 * var * int64'>,\n",
       "  'Height': <Array [[159, 140, 168, 150], ..., [159, 140, ..., 140]] type='4 * var * int64'>,\n",
       "  'Yearly income': <Array [[75000, 80000, 0, 0], ..., [75000, ..., 0]] type='4 * var * int64'>,\n",
       "  'Highest degree/grade': <Array [['BS', 'MS', '12', '9'], ..., ['BS', ...]] type='4 * var * string'>},\n",
       " 'Vehicles': {'Household ID': <Array [[0, 0, 0, 0, 0, 0], [2], [3, 3]] type='3 * var * int64'>,\n",
       "  'Type of vehicle': <Array [['Car', 'Car', ..., 'Bike', 'Bike'], ...] type='3 * var * string'>,\n",
       "  '# of riders': <Array [[4, 5, 1, 1, 1, 1], [7], [2, 7]] type='3 * var * int64'>,\n",
       "  'Gas/electric/human powered': <Array [['Gas', 'Electric', ..., 'Human'], ..., [...]] type='3 * var * string'>,\n",
       "  'Year': <Array [[2005, 2018, 2015, 2015, 2015, 2015], ...] type='3 * var * int64'>,\n",
       "  'Cost': <Array [[25000, 40000, 500, 500, 500, 500], ..., [...]] type='3 * var * int64'>}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = os.path.join('/', 'home', 'nfranz', 'research', 'hepfile', 'examples', '*.csv')\n",
    "files = glob.glob(datapath)\n",
    "common_key = 'Household ID'\n",
    "group_names = ['Residences', 'People', 'Vehicles']\n",
    "\n",
    "if group_names is None:\n",
    "    group_names = [os.path.split(file) for file in files]\n",
    "\n",
    "for_ak = {}\n",
    "for f, group_name in zip(files, group_names):\n",
    "    \n",
    "    csv = pd.read_csv(f)\n",
    "    \n",
    "    groups = csv.groupby(common_key)\n",
    "    split_groups = []\n",
    "    for item in groups.groups:\n",
    "        split_groups.append(groups.get_group(item))\n",
    "    \n",
    "    subdict = {}\n",
    "    for grouping in split_groups:\n",
    "        for colname in grouping.columns:\n",
    "            if colname in subdict.keys():\n",
    "                subdict[colname].append(list(grouping[colname].values))\n",
    "            else:\n",
    "                subdict[colname] = [list(grouping[colname].values)]\n",
    "                \n",
    "    for key in subdict.keys():\n",
    "        subdict[key] = ak.Array(subdict[key])\n",
    "        \n",
    "    \n",
    "    for_ak[group_name] = subdict\n",
    "for_ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f2a7c7-3c94-4db5-af8e-c99f70d184ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding group \u001b[1mResidences\u001b[0m\n",
      "Adding a counter for \u001b[1mResidences\u001b[0m as \u001b[1mnResidences\u001b[0m\n",
      "----------------------------------------------------\n",
      "Slashes / are not allowed in dataset names\n",
      "Replacing / with - in dataset name House/apartment/condo\n",
      "The new name will be House-apartment-condo\n",
      "----------------------------------------------------\n",
      "Adding dataset \u001b[1mHousehold ID\u001b[0m to the dictionary under group \u001b[1mResidences\u001b[0m.\n",
      "Adding dataset \u001b[1mHouse-apartment-condo\u001b[0m to the dictionary under group \u001b[1mResidences\u001b[0m.\n",
      "Adding dataset \u001b[1m# of bedrooms\u001b[0m to the dictionary under group \u001b[1mResidences\u001b[0m.\n",
      "Adding dataset \u001b[1m# of bathrooms\u001b[0m to the dictionary under group \u001b[1mResidences\u001b[0m.\n",
      "Adding dataset \u001b[1mSquare footage\u001b[0m to the dictionary under group \u001b[1mResidences\u001b[0m.\n",
      "Adding dataset \u001b[1mYear built\u001b[0m to the dictionary under group \u001b[1mResidences\u001b[0m.\n",
      "Adding dataset \u001b[1mEstimate\u001b[0m to the dictionary under group \u001b[1mResidences\u001b[0m.\n",
      "Adding group \u001b[1mPeople\u001b[0m\n",
      "Adding a counter for \u001b[1mPeople\u001b[0m as \u001b[1mnPeople\u001b[0m\n",
      "----------------------------------------------------\n",
      "Slashes / are not allowed in dataset names\n",
      "Replacing / with - in dataset name Highest degree/grade\n",
      "The new name will be Highest degree-grade\n",
      "----------------------------------------------------\n",
      "Adding dataset \u001b[1mHousehold ID\u001b[0m to the dictionary under group \u001b[1mPeople\u001b[0m.\n",
      "Adding dataset \u001b[1mFirst name\u001b[0m to the dictionary under group \u001b[1mPeople\u001b[0m.\n",
      "Adding dataset \u001b[1mLast name\u001b[0m to the dictionary under group \u001b[1mPeople\u001b[0m.\n",
      "Adding dataset \u001b[1mGender ID\u001b[0m to the dictionary under group \u001b[1mPeople\u001b[0m.\n",
      "Adding dataset \u001b[1mAge\u001b[0m to the dictionary under group \u001b[1mPeople\u001b[0m.\n",
      "Adding dataset \u001b[1mHeight\u001b[0m to the dictionary under group \u001b[1mPeople\u001b[0m.\n",
      "Adding dataset \u001b[1mYearly income\u001b[0m to the dictionary under group \u001b[1mPeople\u001b[0m.\n",
      "Adding dataset \u001b[1mHighest degree-grade\u001b[0m to the dictionary under group \u001b[1mPeople\u001b[0m.\n",
      "Adding group \u001b[1mVehicles\u001b[0m\n",
      "Adding a counter for \u001b[1mVehicles\u001b[0m as \u001b[1mnVehicles\u001b[0m\n",
      "----------------------------------------------------\n",
      "Slashes / are not allowed in dataset names\n",
      "Replacing / with - in dataset name Gas/electric/human powered\n",
      "The new name will be Gas-electric-human powered\n",
      "----------------------------------------------------\n",
      "Adding dataset \u001b[1mHousehold ID\u001b[0m to the dictionary under group \u001b[1mVehicles\u001b[0m.\n",
      "Adding dataset \u001b[1mType of vehicle\u001b[0m to the dictionary under group \u001b[1mVehicles\u001b[0m.\n",
      "Adding dataset \u001b[1m# of riders\u001b[0m to the dictionary under group \u001b[1mVehicles\u001b[0m.\n",
      "Adding dataset \u001b[1mGas-electric-human powered\u001b[0m to the dictionary under group \u001b[1mVehicles\u001b[0m.\n",
      "Adding dataset \u001b[1mYear\u001b[0m to the dictionary under group \u001b[1mVehicles\u001b[0m.\n",
      "Adding dataset \u001b[1mCost\u001b[0m to the dictionary under group \u001b[1mVehicles\u001b[0m.\n",
      "Writing the hdf5 file from the awkward array...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfranz/anaconda3/lib/python3.9/site-packages/hepfile/write.py:560: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if group == \"_SINGLETONS_GROUP_\" and dataset is not \"COUNTER\":\n",
      "/home/nfranz/anaconda3/lib/python3.9/site-packages/hepfile/write.py:560: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if group == \"_SINGLETONS_GROUP_\" and dataset is not \"COUNTER\":\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'House'\n\nThis error occurred while calling\n\n    numpy.asarray(\n        <Array ['House', 'Apartment', 'Condo', 'House'] type='4 * string'>\n        dtype[float64]-instance\n    )",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/awkward/highlevel.py:1289\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ak\u001b[38;5;241m.\u001b[39m_errors\u001b[38;5;241m.\u001b[39mOperationErrorContext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.asarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, arguments):\n\u001b[0;32m-> 1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mak\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/awkward/_connect/numpy.py:41\u001b[0m, in \u001b[0;36mconvert_to_array\u001b[0;34m(layout, args, kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'House'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawkward_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawkward_to_hepfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfor_ak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtesting_csv_translation.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hepfile/awkward_tools.py:119\u001b[0m, in \u001b[0;36mawkward_to_hepfile\u001b[0;34m(ak_array, outfile, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_SINGLETONS_GROUP_/COUNTER\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_GROUPS_\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_SINGLETONS_GROUP_\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting the hdf5 file from the awkward array...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m hdfile \u001b[38;5;241m=\u001b[39m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mforce_single_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hepfile/write.py:590\u001b[0m, in \u001b[0;36mwrite_to_file\u001b[0;34m(filename, data, comp_type, comp_opts, force_single_precision, verbose)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mWriting to file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 590\u001b[0m     \u001b[43mhdoutfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomp_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression_opts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomp_opts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_dtype\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# For writing strings, we need to make sure our strings are ascii and not Unicode\u001b[39;00m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See my question on StackOverflow and the super-helpful response!\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;66;03m# https://stackoverflow.com/questions/68500454/can-i-use-h5py-to-write-strings-to-an-hdf5-file-in-one-line-rather-than-looping\u001b[39;00m\n\u001b[1;32m    599\u001b[0m     dataset_dtype \u001b[38;5;241m=\u001b[39m h5\u001b[38;5;241m.\u001b[39mspecial_dtype(vlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/h5py/_hl/group.py:161\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    158\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    159\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 161\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py:48\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[0;32m---> 48\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_for_new_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecified_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Validate shape\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/h5py/_hl/base.py:118\u001b[0m, in \u001b[0;36marray_for_new_object\u001b[0;34m(data, specified_dtype)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     as_dtype \u001b[38;5;241m=\u001b[39m guess_dtype(data)\n\u001b[0;32m--> 118\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# In most cases, this does nothing. But if data was already an array,\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# and as_dtype is a tagged h5py dtype (e.g. for an object array of strings),\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# asarray() doesn't replace its dtype object. This gives it the tagged dtype:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/awkward/highlevel.py:1289\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m arguments\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ak\u001b[38;5;241m.\u001b[39m_errors\u001b[38;5;241m.\u001b[39mOperationErrorContext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.asarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, arguments):\n\u001b[0;32m-> 1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ak\u001b[38;5;241m.\u001b[39m_connect\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mconvert_to_array(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/awkward/_errors.py:56\u001b[0m, in \u001b[0;36mErrorContext.__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Handle caught exception\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exception_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexception_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# `_kwargs` may hold cyclic references, that we really want to avoid\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# as this can lead to large buffers remaining in memory for longer than absolutely necessary\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Let's just clear this, now.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/awkward/_errors.py:71\u001b[0m, in \u001b[0;36mErrorContext.handle_exception\u001b[0;34m(self, cls, exception)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecorate_exception(\u001b[38;5;28mcls\u001b[39m, exception)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecorate_exception(\u001b[38;5;28mcls\u001b[39m, exception)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'House'\n\nThis error occurred while calling\n\n    numpy.asarray(\n        <Array ['House', 'Apartment', 'Condo', 'House'] type='4 * string'>\n        dtype[float64]-instance\n    )"
     ]
    }
   ],
   "source": [
    "hf.awkward_tools.awkward_to_hepfile(for_ak, 'testing_csv_translation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333279de-04ab-4710-a176-cb9b30b81e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
