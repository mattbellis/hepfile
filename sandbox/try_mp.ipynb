{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bccd693-e497-4413-84d1-17bcfb037fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hepfile as hf\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "sys.path.insert(0, os.path.abspath(\"../src/hepfile\"))\n",
    "\n",
    "from src.hepfile.constants import *\n",
    "from errors import *\n",
    "from src.hepfile.read import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c439e3c-c76c-4468-b32e-03b30725aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_subset(packed_args) -> dict:\n",
    "    \n",
    "    '''\n",
    "    Reads a subset of the data, from the HDF5 file to fill a data dictionary.\n",
    "\n",
    "    Args:\n",
    "\tpacked_args (list): list with 1 x 4 dimensions in the order: infile, subset, verbose, desired_groups. \n",
    "                        subset, verbose, and desired_groups should be the same for all rows\n",
    "    \n",
    "    Returns:\n",
    "\tdata (dict): Selected data from HDF5 file\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # unpack args    \n",
    "    infile, subset, verbose, desired_groups = zip(*packed_args)\n",
    "    \n",
    "    # Create the initial data and bucket dictionary to hold the data\n",
    "    data = {}\n",
    "    bucket = {}\n",
    "\n",
    "    # We'll fill the data dictionary with some extra fields, though we won't\n",
    "    # need them all for the bucket\n",
    "    data[\"_MAP_DATASETS_TO_COUNTERS_\"] = {}\n",
    "    data[\"_MAP_DATASETS_TO_INDEX_\"] = {}\n",
    "    data[\"_LIST_OF_COUNTERS_\"] = []\n",
    "    data[\"_LIST_OF_DATASETS_\"] = []\n",
    "    data[\"_META_\"] = {}\n",
    "\n",
    "    # Get the number of buckets.\n",
    "    # In HEP (High Energy Physics), this would be the number of events\n",
    "    data[\"_NUMBER_OF_BUCKETS_\"] = infile.attrs[\"_NUMBER_OF_BUCKETS_\"]\n",
    "\n",
    "    if type(subset) is tuple:\n",
    "        subset = list(subset)\n",
    "\n",
    "    if type(subset) is int:\n",
    "        print(\"Single subset value of {subset} being interpreted as a high range\")\n",
    "        print(f\"subset being set to a range of (0,{subset})\\n\")\n",
    "        subset = [0, subset]\n",
    "\n",
    "    # If the user has specified `subset` incorrectly, then let's return\n",
    "    # an empty data and bucket\n",
    "    if subset[1]-subset[0]<=0:\n",
    "        raise RangeSubsetError(f\"The range in subset is either 0 or negative! {subset[1]} - {subset[0]} = {subset[1] - subset[0]}\")\n",
    "\n",
    "    # Make sure the user is not asking for something bigger than the file!\n",
    "    nbuckets = data[\"_NUMBER_OF_BUCKETS_\"]\n",
    "\n",
    "    if subset[0] > nbuckets:\n",
    "        raise RangeSubsetError(f'Range for subset starts greater than number of buckets in file! {subset[0]} > {nbuckets}')\n",
    "\n",
    "    if subset[1] > nbuckets:\n",
    "        warnings.warn(f'Range for subset is greater than number of buckets in file!\\n{subset[1]} > {nbuckets}\\nHigh range of subset will be set to {nbuckets}\\n')\n",
    "        subset[1] = nbuckets\n",
    "\n",
    "    data[\"_NUMBER_OF_BUCKETS_\"] = subset[1] - subset[0]\n",
    "\n",
    "    nbuckets = data[\"_NUMBER_OF_BUCKETS_\"]\n",
    "\n",
    "    ############################################################################\n",
    "    # Get the datasets and counters\n",
    "    ############################################################################\n",
    "    dc = infile[\"_MAP_DATASETS_TO_COUNTERS_\"]\n",
    "    for vals in dc:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Map datasets to counters: {vals}\")\n",
    "\n",
    "        # The decode is there because vals were stored as numpy.bytes\n",
    "        counter = vals[1].decode()\n",
    "        index = f\"{counter}_INDEX\"\n",
    "        data[\"_MAP_DATASETS_TO_COUNTERS_\"][vals[0].decode()] = counter\n",
    "        data[\"_MAP_DATASETS_TO_INDEX_\"][vals[0].decode()] = index\n",
    "        data[\"_LIST_OF_COUNTERS_\"].append(vals[1].decode())\n",
    "        data[\"_LIST_OF_DATASETS_\"].append(vals[0].decode())\n",
    "        data[\"_LIST_OF_DATASETS_\"].append(vals[1].decode())  # Get the counters as well\n",
    "\n",
    "    # We may have added some counters and datasets multiple times.\n",
    "    # So just to be sure, only keep the unique values\n",
    "    data[\"_LIST_OF_COUNTERS_\"] = np.unique(data[\"_LIST_OF_COUNTERS_\"]).tolist()\n",
    "    data[\"_LIST_OF_DATASETS_\"] = np.unique(data[\"_LIST_OF_DATASETS_\"]).tolist()\n",
    "    ############################################################################            \n",
    "\n",
    "    ############################################################################\n",
    "    # Pull out the SINGLETON datasets\n",
    "    ############################################################################\n",
    "    sg = infile[\"_SINGLETONSGROUPFORSTORAGE_\"][0]  # This is a numpy array of strings\n",
    "    decoded_string = sg[1].decode()\n",
    "\n",
    "    vals = decoded_string.split(\"__:__\")\n",
    "    vals.remove(\"COUNTER\")\n",
    "\n",
    "    data[\"_SINGLETONS_GROUP_\"] = vals\n",
    "    ############################################################################\n",
    "\n",
    "    ############################################################################\n",
    "    # Get the list of datasets and groups\n",
    "    ############################################################################\n",
    "    all_datasets = data[\"_LIST_OF_DATASETS_\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"all_datasets: {all_datasets}\")\n",
    "    ############################################################################\n",
    "\n",
    "    ############################################################################\n",
    "    # Only keep select data from file, if we have specified desired_groups\n",
    "    ############################################################################\n",
    "    if desired_groups is not None:\n",
    "        if type(desired_groups) != list:\n",
    "            desired_groups = list(desired_groups)\n",
    "\n",
    "        # Count backwards because we'll be removing stuff as we go.\n",
    "        i = len(all_datasets) - 1\n",
    "        while i >= 0:\n",
    "            entry = all_datasets[i]\n",
    "\n",
    "            is_dropped = True\n",
    "            # This is looking to see if the string is anywhere in the name\n",
    "            # of the dataset\n",
    "            for desdat in desired_groups:\n",
    "                if desdat in entry:\n",
    "                    is_dropped = False\n",
    "                    break\n",
    "\n",
    "            if is_dropped == True:\n",
    "                print(f\"Not reading out {entry} from the file....\")\n",
    "                all_datasets.remove(entry)\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"After only selecting certain datasets ----- \")\n",
    "            print(f\"all_datasets: {all_datasets}\")\n",
    "    ###########################################################################\n",
    "\n",
    "    # We might need the counter for SINGLETONS so let's pull it out\n",
    "    data[\"_SINGLETONS_GROUP_/COUNTER\"] = infile[\"_SINGLETONS_GROUP_\"]['COUNTER']\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\nDatasets and counters:\")\n",
    "        print(data[\"_MAP_DATASETS_TO_COUNTERS_\"])\n",
    "        print(\"\\nList of counters:\")\n",
    "        print(data[\"_LIST_OF_COUNTERS_\"])\n",
    "        print(\"\\n_SINGLETONS_GROUP_/COUNTER:\")\n",
    "        print(data[\"_SINGLETONS_GROUP_/COUNTER\"])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    ############################################################################\n",
    "    # Pull out the counters and build the indices\n",
    "    ############################################################################\n",
    "    print(\"Building the indices...\\n\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"data.keys()\")\n",
    "        print(data.keys())\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # We will need to keep track of the indices in the entire file\n",
    "    # This way, if the user specifies a subset of the data, we have the full \n",
    "    # indices already calculated\n",
    "    full_file_indices = {}\n",
    "\n",
    "    for counter_name in data[\"_LIST_OF_COUNTERS_\"]:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"counter name: ------------ {counter_name}\\n\")\n",
    "\n",
    "        full_file_counters = infile[counter_name]\n",
    "        full_file_index = calculate_index_from_counters(full_file_counters)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"full file counters: {full_file_counters}\\n\")\n",
    "            print(f\"full file index: {full_file_index}\\n\")\n",
    "\n",
    "        # If we passed in subset, grab that slice of the data from the file\n",
    "        if subset is not None and subset[1] <= subset[0]:\n",
    "            raise RangeSubsetError(f\"Unable to read anything in! High range of {subset[1]} is less than or equal to low range of {subset[0]}\")\n",
    "\n",
    "        elif subset is not None:\n",
    "            # We tack on +1 to the high range of subset when we pull out the counters\n",
    "            # and index because we want to get all of the entries for the last entry.\n",
    "            data[counter_name] = infile[counter_name][subset[0] : subset[1]+1]\n",
    "            index = full_file_index[subset[0] : subset[1]+1]\n",
    "        else:\n",
    "            data[counter_name] = infile[counter_name][:]\n",
    "            index = full_file_index\n",
    "\n",
    "        subset_index = index\n",
    "        # If the file is *not* empty....\n",
    "        # Just to make sure the \"local\" index of the data dictionary starts at 0\n",
    "        if len(index)>0:\n",
    "            subset_index = index - index[0]\n",
    "\n",
    "        index_name = \"%s_INDEX\" % (counter_name)\n",
    "\n",
    "        data[index_name] = subset_index\n",
    "        full_file_indices[index_name] = index\n",
    "\n",
    "    print(\"Built the indices!\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"full_file_index: \")\n",
    "        print(f\"{full_file_indices}\\n\")\n",
    "\n",
    "    # Loop over the all_datasets we want and pull out the data.\n",
    "    for name in all_datasets:\n",
    "\n",
    "        # If this is a counter, we're going to have to grab the indices\n",
    "        # differently than for a \"normal\" dataset\n",
    "        IS_COUNTER = True\n",
    "        index_name = None\n",
    "        if name not in data[\"_LIST_OF_COUNTERS_\"]:\n",
    "            index_name = data[\"_MAP_DATASETS_TO_INDEX_\"][name]\n",
    "            IS_COUNTER = False # We will use different indices for the counters\n",
    "\n",
    "        if verbose == True:\n",
    "            print(f\"------ {name}\")\n",
    "            print(f\"index_name: {index_name}\\n\")\n",
    "\n",
    "        dataset = infile[name]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"dataset type: {type(dataset)}\")\n",
    "\n",
    "        # This will ignore the groups\n",
    "        if type(dataset) == h5.Dataset:\n",
    "            dataset_name = name\n",
    "\n",
    "            if subset is not None:\n",
    "                if IS_COUNTER:\n",
    "                    # If this is a counter, then the subset indices\n",
    "                    # map on to the same locations for any counters\n",
    "                    lo = subset[0]\n",
    "                    hi = subset[1]\n",
    "                else:\n",
    "                    lo = full_file_indices[index_name][0]\n",
    "                    hi = full_file_indices[index_name][-1]\n",
    "                if verbose:\n",
    "                    print(f\"dataset name/lo/hi: {dataset_name},{lo},{hi}\\n\")\n",
    "                data[dataset_name] = dataset[lo : hi]\n",
    "            else:\n",
    "                data[dataset_name] = dataset[:]\n",
    "\n",
    "            bucket[dataset_name] = None  # This will be filled for individual bucket\n",
    "            if verbose == True:\n",
    "                print(dataset)\n",
    "\n",
    "        # write the metadata for that group to data if it exists\n",
    "        if name not in constants.protected_names and 'meta' in dataset.attrs.keys():\n",
    "            data['_META_'][name] = dataset.attrs['meta']\n",
    "                        \n",
    "    print(\"Data is read in and input file is closed.\")\n",
    "\n",
    "    # edit data so it matches the format of the data dict that was saved to the file\n",
    "    # this makes it so that data can be directly passed to write_to_file\n",
    "    # 1) add back in _GROUP_\n",
    "    datasets = np.array(data['_LIST_OF_DATASETS_'])\n",
    "\n",
    "    allgroups = np.array([d.split('/')[0] for d in datasets])\n",
    "    \n",
    "    singletons_group = set(data['_SINGLETONS_GROUP_'])\n",
    "    groups = {}\n",
    "    \n",
    "    groups['_SINGLETONS_GROUP_'] = data['_SINGLETONS_GROUP_'] # copy over the data\n",
    "    \n",
    "    for key in np.unique(allgroups):\n",
    "\n",
    "        if key in singletons_group: continue\n",
    "        if key in constants.protected_names: continue\n",
    "        \n",
    "        where_groups = np.where((key == allgroups) * (key != datasets))[0]\n",
    "        groups[key] = [dataset.split('/')[-1] for dataset in datasets[where_groups]]\n",
    "        \n",
    "    data['_GROUPS_'] = groups\n",
    "\n",
    "    # 2) add back in _MAP_DATASETS_TO_DATA_TYPES\n",
    "    dtypes = {}\n",
    "    for key in data['_LIST_OF_DATASETS_']:\n",
    "\n",
    "        if key not in data.keys():\n",
    "            continue\n",
    "        \n",
    "        if isinstance(data[key], list):\n",
    "            data[key] = np.array(data[key])\n",
    "\n",
    "        dtypes[key] = data[key].dtype\n",
    "        \n",
    "    data['_MAP_DATASETS_TO_DATA_TYPES_'] = dtypes\n",
    "\n",
    "    return data, bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3aeb8fa4-95b6-493d-b32e-c7ade644d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename:str, num_cores:int=mp.cpu_count(), verbose:bool=False, desired_groups:list[str]=None, subset:int=None, return_awkward:bool=False) -> tuple[dict, dict]:\n",
    "    '''\n",
    "    Reads all, or a subset of the data, from the HDF5 file to fill a data dictionary.\n",
    "    Returns an empty dictionary to be filled later with data from individual buckets.\n",
    "\n",
    "    Args:\n",
    "\tfilename (string): Name of the input file\n",
    "\t\n",
    "\tverbose (boolean): True if debug output is required\n",
    "\n",
    "\tdesired_groups (list): Groups to be read from input file, \n",
    "\n",
    "\tsubset (int): Number of buckets to be read from input file\n",
    "\n",
    "        return_awkward (boolean): If True, returns an awkward array Record. Default is False\n",
    "\n",
    "    Returns:\n",
    "\tdata (dict): Selected data from HDF5 file\n",
    "\t\n",
    "\tbucket (dict): An empty bucket dictionary to be filled by data from select buckets\n",
    "\n",
    "    '''\n",
    "    \n",
    "    with h5.File(filename, 'r+') as infile:\n",
    "        \n",
    "        nbuckets = infile.attrs[\"_NUMBER_OF_BUCKETS_\"]\n",
    "        if subset is None:\n",
    "            subset = (0, nbuckets)\n",
    "        \n",
    "        # check number of cores\n",
    "        if num_cores > nbuckets:\n",
    "            warnings.warn('num_cores is greater than nbuckets, reducing number of cores used to the number of buckets!')\n",
    "            num_cores = nbuckets\n",
    "        \n",
    "        # pack up the arguments to pass to multiprocessing pool\n",
    "        per_core = np.ceil((subset[1] - subset[0]) / num_cores)\n",
    "        all_subsets = []\n",
    "        min_val = subset[0]\n",
    "        max_val = -1\n",
    "        while max_val < subset[1]:\n",
    "            max_val = min_val + per_core\n",
    "            if max_val > nbuckets:\n",
    "                max_val = nbuckets\n",
    "            all_subsets.append((int(min_val), int(max_val)))\n",
    "            min_val += per_core\n",
    "        \n",
    "        n = len(all_subsets)\n",
    "        packed_args = list(zip([infile]*n, all_subsets, [verbose]*n, [desired_groups]*n))\n",
    "        \n",
    "        with mp.Pool(num_cores) as p:\n",
    "            out = p.apply(_load_subset, packed_args)\n",
    "        \n",
    "        #data, bucket = _load_subset(infile, verbose=verbose, desired_groups=desired_groups, subset=subset)\n",
    "        print(out)\n",
    "#     if return_awkward:\n",
    "#         from hepfile.awkward_tools import hepfile_to_awkward\n",
    "#         return hepfile_to_awkward(data), bucket\n",
    "    \n",
    "#     return data, bucket\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa9fcb57-22e8-4905-b94f-6e769a90eef2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "h5py objects cannot be pickled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../docs/example_nb/updated-awkward-array.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m data, bucket \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, num_cores, verbose, desired_groups, subset, return_awkward)\u001b[0m\n\u001b[1;32m     48\u001b[0m packed_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m([infile]\u001b[38;5;241m*\u001b[39mn, all_subsets, [verbose]\u001b[38;5;241m*\u001b[39mn, [desired_groups]\u001b[38;5;241m*\u001b[39mn))\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(num_cores) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m---> 51\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_load_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacked_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#data, bucket = _load_subset(infile, verbose=verbose, desired_groups=desired_groups, subset=subset)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py:357\u001b[0m, in \u001b[0;36mPool.apply\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39m(), kwds\u001b[38;5;241m=\u001b[39m{}):\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    Equivalent of `func(*args, **kwds)`.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Pool must be running.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/pool.py:537\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     \u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     job, idx \u001b[38;5;241m=\u001b[39m task[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/h5py/_hl/base.py:368\u001b[0m, in \u001b[0;36mHLObject.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getnewargs__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Disable pickle.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Handles for HDF5 objects can't be reliably deserialised, because the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    limitations, look at the h5pickle project on PyPI.\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py objects cannot be pickled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: h5py objects cannot be pickled"
     ]
    }
   ],
   "source": [
    "filepath = os.path.abspath('../docs/example_nb/updated-awkward-array.h5')\n",
    "data, bucket = load(filepath, num_cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e070e494-271c-474d-adb5-1240aa7ea4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_MAP_DATASETS_TO_COUNTERS_': {'_SINGLETONS_GROUP_': '_SINGLETONS_GROUP_/COUNTER',\n",
       "  'nParticles': '_SINGLETONS_GROUP_/COUNTER',\n",
       "  'jet': 'jet/njet',\n",
       "  'jet/px': 'jet/njet',\n",
       "  'jet/py': 'jet/njet',\n",
       "  'muons': 'muons/nmuons',\n",
       "  'muons/px': 'muons/nmuons',\n",
       "  'muons/py': 'muons/nmuons'},\n",
       " '_MAP_DATASETS_TO_INDEX_': {'_SINGLETONS_GROUP_': '_SINGLETONS_GROUP_/COUNTER_INDEX',\n",
       "  'nParticles': '_SINGLETONS_GROUP_/COUNTER_INDEX',\n",
       "  'jet': 'jet/njet_INDEX',\n",
       "  'jet/px': 'jet/njet_INDEX',\n",
       "  'jet/py': 'jet/njet_INDEX',\n",
       "  'muons': 'muons/nmuons_INDEX',\n",
       "  'muons/px': 'muons/nmuons_INDEX',\n",
       "  'muons/py': 'muons/nmuons_INDEX'},\n",
       " '_LIST_OF_COUNTERS_': ['_SINGLETONS_GROUP_/COUNTER',\n",
       "  'jet/njet',\n",
       "  'muons/nmuons'],\n",
       " '_LIST_OF_DATASETS_': ['_SINGLETONS_GROUP_',\n",
       "  '_SINGLETONS_GROUP_/COUNTER',\n",
       "  'jet',\n",
       "  'jet/njet',\n",
       "  'jet/px',\n",
       "  'jet/py',\n",
       "  'muons',\n",
       "  'muons/nmuons',\n",
       "  'muons/px',\n",
       "  'muons/py',\n",
       "  'nParticles'],\n",
       " '_META_': {},\n",
       " '_NUMBER_OF_BUCKETS_': 2,\n",
       " '_SINGLETONS_GROUP_': array(['nParticles'], dtype='<U10'),\n",
       " '_SINGLETONS_GROUP_/COUNTER': array([1, 1]),\n",
       " '_SINGLETONS_GROUP_/COUNTER_INDEX': array([0, 1, 2]),\n",
       " 'jet/njet': array([3, 4]),\n",
       " 'jet/njet_INDEX': array([0, 3, 7]),\n",
       " 'muons/nmuons': array([3, 4]),\n",
       " 'muons/nmuons_INDEX': array([0, 3, 7]),\n",
       " 'jet/px': array([1, 2, 3, 3, 4, 6, 7]),\n",
       " 'jet/py': array([1, 2, 3, 3, 4, 6, 7]),\n",
       " 'muons/px': array([1, 2, 3, 3, 4, 6, 7]),\n",
       " 'muons/py': array([1, 2, 3, 3, 4, 6, 7]),\n",
       " 'nParticles': array([3, 4]),\n",
       " '_GROUPS_': {'_SINGLETONS_GROUP_': ['nParticles'],\n",
       "  'jet': ['njet', 'px', 'py'],\n",
       "  'muons': ['nmuons', 'px', 'py']},\n",
       " '_MAP_DATASETS_TO_DATA_TYPES_': {'_SINGLETONS_GROUP_': dtype('<U10'),\n",
       "  '_SINGLETONS_GROUP_/COUNTER': dtype('int64'),\n",
       "  'jet/njet': dtype('int64'),\n",
       "  'jet/px': dtype('int64'),\n",
       "  'jet/py': dtype('int64'),\n",
       "  'muons/nmuons': dtype('int64'),\n",
       "  'muons/px': dtype('int64'),\n",
       "  'muons/py': dtype('int64'),\n",
       "  'nParticles': dtype('int64')}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcaa42f-82d5-4b7c-ac85-ac0b309ce19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
