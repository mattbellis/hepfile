{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bccd693-e497-4413-84d1-17bcfb037fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hepfile as hf\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "sys.path.insert(0, os.path.abspath(\"../src/hepfile\"))\n",
    "\n",
    "from src.hepfile.constants import *\n",
    "from errors import *\n",
    "from src.hepfile.read import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c439e3c-c76c-4468-b32e-03b30725aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_subset(infile:h5.File, subset:tuple, verbose:bool=False, desired_groups:list[str]=None) -> dict:\n",
    "\n",
    "    '''\n",
    "    Reads a subset of the data, from the HDF5 file to fill a data dictionary.\n",
    "\n",
    "    Args:\n",
    "\tinfile (h5py.File): h5py.File input file\n",
    "\t\n",
    "\tverbose (boolean): True if debug output is required\n",
    "\n",
    "\tdesired_groups (list): Groups to be read from input file, \n",
    "\n",
    "\tsubset (int): Number of buckets to be read from input file\n",
    "    \n",
    "    Returns:\n",
    "\tdata (dict): Selected data from HDF5 file\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Create the initial data and bucket dictionary to hold the data\n",
    "    data = {}\n",
    "    bucket = {}\n",
    "\n",
    "    # We'll fill the data dictionary with some extra fields, though we won't\n",
    "    # need them all for the bucket\n",
    "    data[\"_MAP_DATASETS_TO_COUNTERS_\"] = {}\n",
    "    data[\"_MAP_DATASETS_TO_INDEX_\"] = {}\n",
    "    data[\"_LIST_OF_COUNTERS_\"] = []\n",
    "    data[\"_LIST_OF_DATASETS_\"] = []\n",
    "    data[\"_META_\"] = {}\n",
    "\n",
    "    # Get the number of buckets.\n",
    "    # In HEP (High Energy Physics), this would be the number of events\n",
    "    data[\"_NUMBER_OF_BUCKETS_\"] = infile.attrs[\"_NUMBER_OF_BUCKETS_\"]\n",
    "\n",
    "    if type(subset) is tuple:\n",
    "        subset = list(subset)\n",
    "\n",
    "    if type(subset) is int:\n",
    "        print(\"Single subset value of {subset} being interpreted as a high range\")\n",
    "        print(f\"subset being set to a range of (0,{subset})\\n\")\n",
    "        subset = [0, subset]\n",
    "\n",
    "    # If the user has specified `subset` incorrectly, then let's return\n",
    "    # an empty data and bucket\n",
    "    if subset[1]-subset[0]<=0:\n",
    "        raise RangeSubsetError(f\"The range in subset is either 0 or negative! {subset[1]} - {subset[0]} = {subset[1] - subset[0]}\")\n",
    "\n",
    "    # Make sure the user is not asking for something bigger than the file!\n",
    "    nbuckets = data[\"_NUMBER_OF_BUCKETS_\"]\n",
    "\n",
    "    if subset[0] > nbuckets:\n",
    "        raise RangeSubsetError(f'Range for subset starts greater than number of buckets in file! {subset[0]} > {nbuckets}')\n",
    "\n",
    "    if subset[1] > nbuckets:\n",
    "        warnings.warn(f'Range for subset is greater than number of buckets in file!\\n{subset[1]} > {nbuckets}\\nHigh range of subset will be set to {nbuckets}\\n')\n",
    "        subset[1] = nbuckets\n",
    "\n",
    "    data[\"_NUMBER_OF_BUCKETS_\"] = subset[1] - subset[0]\n",
    "\n",
    "    nbuckets = data[\"_NUMBER_OF_BUCKETS_\"]\n",
    "\n",
    "    ############################################################################\n",
    "    # Get the datasets and counters\n",
    "    ############################################################################\n",
    "    dc = infile[\"_MAP_DATASETS_TO_COUNTERS_\"]\n",
    "    for vals in dc:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Map datasets to counters: {vals}\")\n",
    "\n",
    "        # The decode is there because vals were stored as numpy.bytes\n",
    "        counter = vals[1].decode()\n",
    "        index = f\"{counter}_INDEX\"\n",
    "        data[\"_MAP_DATASETS_TO_COUNTERS_\"][vals[0].decode()] = counter\n",
    "        data[\"_MAP_DATASETS_TO_INDEX_\"][vals[0].decode()] = index\n",
    "        data[\"_LIST_OF_COUNTERS_\"].append(vals[1].decode())\n",
    "        data[\"_LIST_OF_DATASETS_\"].append(vals[0].decode())\n",
    "        data[\"_LIST_OF_DATASETS_\"].append(vals[1].decode())  # Get the counters as well\n",
    "\n",
    "    # We may have added some counters and datasets multiple times.\n",
    "    # So just to be sure, only keep the unique values\n",
    "    data[\"_LIST_OF_COUNTERS_\"] = np.unique(data[\"_LIST_OF_COUNTERS_\"]).tolist()\n",
    "    data[\"_LIST_OF_DATASETS_\"] = np.unique(data[\"_LIST_OF_DATASETS_\"]).tolist()\n",
    "    ############################################################################            \n",
    "\n",
    "    ############################################################################\n",
    "    # Pull out the SINGLETON datasets\n",
    "    ############################################################################\n",
    "    sg = infile[\"_SINGLETONSGROUPFORSTORAGE_\"][0]  # This is a numpy array of strings\n",
    "    decoded_string = sg[1].decode()\n",
    "\n",
    "    vals = decoded_string.split(\"__:__\")\n",
    "    vals.remove(\"COUNTER\")\n",
    "\n",
    "    data[\"_SINGLETONS_GROUP_\"] = vals\n",
    "    ############################################################################\n",
    "\n",
    "    ############################################################################\n",
    "    # Get the list of datasets and groups\n",
    "    ############################################################################\n",
    "    all_datasets = data[\"_LIST_OF_DATASETS_\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"all_datasets: {all_datasets}\")\n",
    "    ############################################################################\n",
    "\n",
    "    ############################################################################\n",
    "    # Only keep select data from file, if we have specified desired_groups\n",
    "    ############################################################################\n",
    "    if desired_groups is not None:\n",
    "        if type(desired_groups) != list:\n",
    "            desired_groups = list(desired_groups)\n",
    "\n",
    "        # Count backwards because we'll be removing stuff as we go.\n",
    "        i = len(all_datasets) - 1\n",
    "        while i >= 0:\n",
    "            entry = all_datasets[i]\n",
    "\n",
    "            is_dropped = True\n",
    "            # This is looking to see if the string is anywhere in the name\n",
    "            # of the dataset\n",
    "            for desdat in desired_groups:\n",
    "                if desdat in entry:\n",
    "                    is_dropped = False\n",
    "                    break\n",
    "\n",
    "            if is_dropped == True:\n",
    "                print(f\"Not reading out {entry} from the file....\")\n",
    "                all_datasets.remove(entry)\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"After only selecting certain datasets ----- \")\n",
    "            print(f\"all_datasets: {all_datasets}\")\n",
    "    ###########################################################################\n",
    "\n",
    "    # We might need the counter for SINGLETONS so let's pull it out\n",
    "    data[\"_SINGLETONS_GROUP_/COUNTER\"] = infile[\"_SINGLETONS_GROUP_\"]['COUNTER']\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\nDatasets and counters:\")\n",
    "        print(data[\"_MAP_DATASETS_TO_COUNTERS_\"])\n",
    "        print(\"\\nList of counters:\")\n",
    "        print(data[\"_LIST_OF_COUNTERS_\"])\n",
    "        print(\"\\n_SINGLETONS_GROUP_/COUNTER:\")\n",
    "        print(data[\"_SINGLETONS_GROUP_/COUNTER\"])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    ############################################################################\n",
    "    # Pull out the counters and build the indices\n",
    "    ############################################################################\n",
    "    print(\"Building the indices...\\n\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"data.keys()\")\n",
    "        print(data.keys())\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # We will need to keep track of the indices in the entire file\n",
    "    # This way, if the user specifies a subset of the data, we have the full \n",
    "    # indices already calculated\n",
    "    full_file_indices = {}\n",
    "\n",
    "    for counter_name in data[\"_LIST_OF_COUNTERS_\"]:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"counter name: ------------ {counter_name}\\n\")\n",
    "\n",
    "        full_file_counters = infile[counter_name]\n",
    "        full_file_index = calculate_index_from_counters(full_file_counters)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"full file counters: {full_file_counters}\\n\")\n",
    "            print(f\"full file index: {full_file_index}\\n\")\n",
    "\n",
    "        # If we passed in subset, grab that slice of the data from the file\n",
    "        if subset is not None and subset[1] <= subset[0]:\n",
    "            raise RangeSubsetError(f\"Unable to read anything in! High range of {subset[1]} is less than or equal to low range of {subset[0]}\")\n",
    "\n",
    "        elif subset is not None:\n",
    "            # We tack on +1 to the high range of subset when we pull out the counters\n",
    "            # and index because we want to get all of the entries for the last entry.\n",
    "            data[counter_name] = infile[counter_name][subset[0] : subset[1]+1]\n",
    "            index = full_file_index[subset[0] : subset[1]+1]\n",
    "        else:\n",
    "            data[counter_name] = infile[counter_name][:]\n",
    "            index = full_file_index\n",
    "\n",
    "        subset_index = index\n",
    "        # If the file is *not* empty....\n",
    "        # Just to make sure the \"local\" index of the data dictionary starts at 0\n",
    "        if len(index)>0:\n",
    "            subset_index = index - index[0]\n",
    "\n",
    "        index_name = \"%s_INDEX\" % (counter_name)\n",
    "\n",
    "        data[index_name] = subset_index\n",
    "        full_file_indices[index_name] = index\n",
    "\n",
    "    print(\"Built the indices!\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"full_file_index: \")\n",
    "        print(f\"{full_file_indices}\\n\")\n",
    "\n",
    "    # Loop over the all_datasets we want and pull out the data.\n",
    "    for name in all_datasets:\n",
    "\n",
    "        # If this is a counter, we're going to have to grab the indices\n",
    "        # differently than for a \"normal\" dataset\n",
    "        IS_COUNTER = True\n",
    "        index_name = None\n",
    "        if name not in data[\"_LIST_OF_COUNTERS_\"]:\n",
    "            index_name = data[\"_MAP_DATASETS_TO_INDEX_\"][name]\n",
    "            IS_COUNTER = False # We will use different indices for the counters\n",
    "\n",
    "        if verbose == True:\n",
    "            print(f\"------ {name}\")\n",
    "            print(f\"index_name: {index_name}\\n\")\n",
    "\n",
    "        dataset = infile[name]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"dataset type: {type(dataset)}\")\n",
    "\n",
    "        # This will ignore the groups\n",
    "        if type(dataset) == h5.Dataset:\n",
    "            dataset_name = name\n",
    "\n",
    "            if subset is not None:\n",
    "                if IS_COUNTER:\n",
    "                    # If this is a counter, then the subset indices\n",
    "                    # map on to the same locations for any counters\n",
    "                    lo = subset[0]\n",
    "                    hi = subset[1]\n",
    "                else:\n",
    "                    lo = full_file_indices[index_name][0]\n",
    "                    hi = full_file_indices[index_name][-1]\n",
    "                if verbose:\n",
    "                    print(f\"dataset name/lo/hi: {dataset_name},{lo},{hi}\\n\")\n",
    "                data[dataset_name] = dataset[lo : hi]\n",
    "            else:\n",
    "                data[dataset_name] = dataset[:]\n",
    "\n",
    "            bucket[dataset_name] = None  # This will be filled for individual bucket\n",
    "            if verbose == True:\n",
    "                print(dataset)\n",
    "\n",
    "        # write the metadata for that group to data if it exists\n",
    "        if name not in constants.protected_names and 'meta' in dataset.attrs.keys():\n",
    "            data['_META_'][name] = dataset.attrs['meta']\n",
    "                        \n",
    "    print(\"Data is read in and input file is closed.\")\n",
    "\n",
    "    # edit data so it matches the format of the data dict that was saved to the file\n",
    "    # this makes it so that data can be directly passed to write_to_file\n",
    "    # 1) add back in _GROUP_\n",
    "    datasets = np.array(data['_LIST_OF_DATASETS_'])\n",
    "\n",
    "    allgroups = np.array([d.split('/')[0] for d in datasets])\n",
    "    \n",
    "    singletons_group = set(data['_SINGLETONS_GROUP_'])\n",
    "    groups = {}\n",
    "    \n",
    "    groups['_SINGLETONS_GROUP_'] = data['_SINGLETONS_GROUP_'] # copy over the data\n",
    "    \n",
    "    for key in np.unique(allgroups):\n",
    "\n",
    "        if key in singletons_group: continue\n",
    "        if key in constants.protected_names: continue\n",
    "        \n",
    "        where_groups = np.where((key == allgroups) * (key != datasets))[0]\n",
    "        groups[key] = [dataset.split('/')[-1] for dataset in datasets[where_groups]]\n",
    "        \n",
    "    data['_GROUPS_'] = groups\n",
    "\n",
    "    # 2) add back in _MAP_DATASETS_TO_DATA_TYPES\n",
    "    dtypes = {}\n",
    "    for key in data['_LIST_OF_DATASETS_']:\n",
    "\n",
    "        if key not in data.keys():\n",
    "            continue\n",
    "        \n",
    "        if isinstance(data[key], list):\n",
    "            data[key] = np.array(data[key])\n",
    "\n",
    "        dtypes[key] = data[key].dtype\n",
    "        \n",
    "    data['_MAP_DATASETS_TO_DATA_TYPES_'] = dtypes\n",
    "\n",
    "    return data, bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3aeb8fa4-95b6-493d-b32e-c7ade644d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename:str, num_cores:int=mp.cpu_count(), verbose:bool=False, desired_groups:list[str]=None, subset:int=None, return_awkward:bool=False) -> tuple[dict, dict]:\n",
    "    '''\n",
    "    Reads all, or a subset of the data, from the HDF5 file to fill a data dictionary.\n",
    "    Returns an empty dictionary to be filled later with data from individual buckets.\n",
    "\n",
    "    Args:\n",
    "\tfilename (string): Name of the input file\n",
    "\t\n",
    "\tverbose (boolean): True if debug output is required\n",
    "\n",
    "\tdesired_groups (list): Groups to be read from input file, \n",
    "\n",
    "\tsubset (int): Number of buckets to be read from input file\n",
    "\n",
    "        return_awkward (boolean): If True, returns an awkward array Record. Default is False\n",
    "\n",
    "    Returns:\n",
    "\tdata (dict): Selected data from HDF5 file\n",
    "\t\n",
    "\tbucket (dict): An empty bucket dictionary to be filled by data from select buckets\n",
    "\n",
    "    '''\n",
    "    \n",
    "    with h5.File(filename, 'r+') as infile:\n",
    "        \n",
    "        nbuckets = infile.attrs[\"_NUMBER_OF_BUCKETS_\"]\n",
    "        print(nbuckets)\n",
    "        data, bucket = _load_subset(infile, verbose=verbose, desired_groups=desired_groups, subset=subset)\n",
    "        \n",
    "    if return_awkward:\n",
    "        from hepfile.awkward_tools import hepfile_to_awkward\n",
    "        return hepfile_to_awkward(data), bucket\n",
    "    \n",
    "    return data, bucket\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa9fcb57-22e8-4905-b94f-6e769a90eef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-csv.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m data, bucket \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 28\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, num_cores, verbose, desired_groups, subset, return_awkward)\u001b[0m\n\u001b[1;32m     26\u001b[0m     nbuckets \u001b[38;5;241m=\u001b[39m infile\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_NUMBER_OF_BUCKETS_\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(nbuckets)\n\u001b[0;32m---> 28\u001b[0m     data, bucket \u001b[38;5;241m=\u001b[39m \u001b[43m_load_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesired_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_awkward:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhepfile\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mawkward_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hepfile_to_awkward\n",
      "Cell \u001b[0;32mIn[36], line 46\u001b[0m, in \u001b[0;36m_load_subset\u001b[0;34m(infile, subset, verbose, desired_groups)\u001b[0m\n\u001b[1;32m     42\u001b[0m     subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, subset]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# If the user has specified `subset` incorrectly, then let's return\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# an empty data and bucket\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msubset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m-\u001b[39msubset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RangeSubsetError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe range in subset is either 0 or negative! \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m subset[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Make sure the user is not asking for something bigger than the file!\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "filepath = 'test-csv.h5'\n",
    "data, bucket = load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e070e494-271c-474d-adb5-1240aa7ea4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_MAP_DATASETS_TO_COUNTERS_': {'_SINGLETONS_GROUP_': '_SINGLETONS_GROUP_/COUNTER',\n",
       "  'Residences': 'Residences/nResidences',\n",
       "  'Residences/Household ID': 'Residences/nResidences',\n",
       "  'Residences/First name': 'Residences/nResidences',\n",
       "  'Residences/Last name': 'Residences/nResidences',\n",
       "  'Residences/Gender ID': 'Residences/nResidences',\n",
       "  'Residences/Age': 'Residences/nResidences',\n",
       "  'Residences/Height': 'Residences/nResidences',\n",
       "  'Residences/Yearly income': 'Residences/nResidences',\n",
       "  'Residences/Highest degree-grade': 'Residences/nResidences',\n",
       "  'People': 'People/nPeople',\n",
       "  'People/Household ID': 'People/nPeople',\n",
       "  'People/Type of vehicle': 'People/nPeople',\n",
       "  'People/# of riders': 'People/nPeople',\n",
       "  'People/Gas-electric-human powered': 'People/nPeople',\n",
       "  'People/Year': 'People/nPeople',\n",
       "  'People/Cost': 'People/nPeople',\n",
       "  'Vehicles': 'Vehicles/nVehicles',\n",
       "  'Vehicles/Household ID': 'Vehicles/nVehicles',\n",
       "  'Vehicles/House-apartment-condo': 'Vehicles/nVehicles',\n",
       "  'Vehicles/# of bedrooms': 'Vehicles/nVehicles',\n",
       "  'Vehicles/# of bathrooms': 'Vehicles/nVehicles',\n",
       "  'Vehicles/Square footage': 'Vehicles/nVehicles',\n",
       "  'Vehicles/Year built': 'Vehicles/nVehicles',\n",
       "  'Vehicles/Estimate': 'Vehicles/nVehicles'},\n",
       " '_MAP_DATASETS_TO_INDEX_': {'_SINGLETONS_GROUP_': '_SINGLETONS_GROUP_/COUNTER_INDEX',\n",
       "  'Residences': 'Residences/nResidences_INDEX',\n",
       "  'Residences/Household ID': 'Residences/nResidences_INDEX',\n",
       "  'Residences/First name': 'Residences/nResidences_INDEX',\n",
       "  'Residences/Last name': 'Residences/nResidences_INDEX',\n",
       "  'Residences/Gender ID': 'Residences/nResidences_INDEX',\n",
       "  'Residences/Age': 'Residences/nResidences_INDEX',\n",
       "  'Residences/Height': 'Residences/nResidences_INDEX',\n",
       "  'Residences/Yearly income': 'Residences/nResidences_INDEX',\n",
       "  'Residences/Highest degree-grade': 'Residences/nResidences_INDEX',\n",
       "  'People': 'People/nPeople_INDEX',\n",
       "  'People/Household ID': 'People/nPeople_INDEX',\n",
       "  'People/Type of vehicle': 'People/nPeople_INDEX',\n",
       "  'People/# of riders': 'People/nPeople_INDEX',\n",
       "  'People/Gas-electric-human powered': 'People/nPeople_INDEX',\n",
       "  'People/Year': 'People/nPeople_INDEX',\n",
       "  'People/Cost': 'People/nPeople_INDEX',\n",
       "  'Vehicles': 'Vehicles/nVehicles_INDEX',\n",
       "  'Vehicles/Household ID': 'Vehicles/nVehicles_INDEX',\n",
       "  'Vehicles/House-apartment-condo': 'Vehicles/nVehicles_INDEX',\n",
       "  'Vehicles/# of bedrooms': 'Vehicles/nVehicles_INDEX',\n",
       "  'Vehicles/# of bathrooms': 'Vehicles/nVehicles_INDEX',\n",
       "  'Vehicles/Square footage': 'Vehicles/nVehicles_INDEX',\n",
       "  'Vehicles/Year built': 'Vehicles/nVehicles_INDEX',\n",
       "  'Vehicles/Estimate': 'Vehicles/nVehicles_INDEX'},\n",
       " '_LIST_OF_COUNTERS_': ['People/nPeople',\n",
       "  'Residences/nResidences',\n",
       "  'Vehicles/nVehicles',\n",
       "  '_SINGLETONS_GROUP_/COUNTER'],\n",
       " '_LIST_OF_DATASETS_': ['People',\n",
       "  'People/# of riders',\n",
       "  'People/Cost',\n",
       "  'People/Gas-electric-human powered',\n",
       "  'People/Household ID',\n",
       "  'People/Type of vehicle',\n",
       "  'People/Year',\n",
       "  'People/nPeople',\n",
       "  'Residences',\n",
       "  'Residences/Age',\n",
       "  'Residences/First name',\n",
       "  'Residences/Gender ID',\n",
       "  'Residences/Height',\n",
       "  'Residences/Highest degree-grade',\n",
       "  'Residences/Household ID',\n",
       "  'Residences/Last name',\n",
       "  'Residences/Yearly income',\n",
       "  'Residences/nResidences',\n",
       "  'Vehicles',\n",
       "  'Vehicles/# of bathrooms',\n",
       "  'Vehicles/# of bedrooms',\n",
       "  'Vehicles/Estimate',\n",
       "  'Vehicles/House-apartment-condo',\n",
       "  'Vehicles/Household ID',\n",
       "  'Vehicles/Square footage',\n",
       "  'Vehicles/Year built',\n",
       "  'Vehicles/nVehicles',\n",
       "  '_SINGLETONS_GROUP_',\n",
       "  '_SINGLETONS_GROUP_/COUNTER'],\n",
       " '_META_': {'People': '', 'Residences': '', 'Vehicles': ''},\n",
       " '_NUMBER_OF_BUCKETS_': 4,\n",
       " '_SINGLETONS_GROUP_': array([], dtype=float64),\n",
       " '_SINGLETONS_GROUP_/COUNTER': array([], dtype=float32),\n",
       " 'People/nPeople': array([6, 1, 2]),\n",
       " 'People/nPeople_INDEX': array([0, 6, 7]),\n",
       " 'Residences/nResidences': array([4, 2, 1, 7]),\n",
       " 'Residences/nResidences_INDEX': array([0, 4, 6, 7]),\n",
       " 'Vehicles/nVehicles': array([1, 1, 1, 1]),\n",
       " 'Vehicles/nVehicles_INDEX': array([0, 1, 2, 3]),\n",
       " '_SINGLETONS_GROUP_/COUNTER_INDEX': array([], dtype=float32),\n",
       " 'People/# of riders': array([4, 5, 1, 1, 1, 1, 7]),\n",
       " 'People/Cost': array([25000, 40000,   500,   500,   500,   500, 45000]),\n",
       " 'People/Gas-electric-human powered': array([b'Gas', b'Electric', b'Human', b'Human', b'Human', b'Human',\n",
       "        b'Gas'], dtype=object),\n",
       " 'People/Household ID': array([0, 0, 0, 0, 0, 0, 2]),\n",
       " 'People/Type of vehicle': array([b'Car', b'Car', b'Bike', b'Bike', b'Bike', b'Bike', b'Car'],\n",
       "       dtype=object),\n",
       " 'People/Year': array([2005, 2018, 2015, 2015, 2015, 2015, 2012]),\n",
       " 'Residences/Age': array([54, 52, 18, 14, 32, 27, 65]),\n",
       " 'Residences/First name': array([b'blah', b'blah', b'blah', b'blah', b'blah', b'blah', b'blah'],\n",
       "       dtype=object),\n",
       " 'Residences/Gender ID': array([b'M', b'F', b'NB', b'F', b'M', b'M', b'F'], dtype=object),\n",
       " 'Residences/Height': array([159, 140, 168, 150, 159, 140, 140]),\n",
       " 'Residences/Highest degree-grade': array([b'BS', b'MS', b'12', b'9', b'BS', b'BS', b'BS'], dtype=object),\n",
       " 'Residences/Household ID': array([0, 0, 0, 0, 1, 1, 2]),\n",
       " 'Residences/Last name': array([b'blah', b'blah', b'blah', b'blah', b'blah', b'blah', b'blah'],\n",
       "       dtype=object),\n",
       " 'Residences/Yearly income': array([75000, 80000,     0,     0, 49000, 40000, 40000]),\n",
       " 'Vehicles/# of bathrooms': array([2.5, 2. , 1. ]),\n",
       " 'Vehicles/# of bedrooms': array([4, 2, 2]),\n",
       " 'Vehicles/Estimate': array([250000,   1400, 325000]),\n",
       " 'Vehicles/House-apartment-condo': array([b'House', b'Apartment', b'Condo'], dtype=object),\n",
       " 'Vehicles/Household ID': array([0, 1, 2]),\n",
       " 'Vehicles/Square footage': array([1500, 1200, 1000]),\n",
       " 'Vehicles/Year built': array([1955, 2002, 2014]),\n",
       " '_GROUPS_': {'_SINGLETONS_GROUP_': [],\n",
       "  'People': ['# of riders',\n",
       "   'Cost',\n",
       "   'Gas-electric-human powered',\n",
       "   'Household ID',\n",
       "   'Type of vehicle',\n",
       "   'Year',\n",
       "   'nPeople'],\n",
       "  'Residences': ['Age',\n",
       "   'First name',\n",
       "   'Gender ID',\n",
       "   'Height',\n",
       "   'Highest degree-grade',\n",
       "   'Household ID',\n",
       "   'Last name',\n",
       "   'Yearly income',\n",
       "   'nResidences'],\n",
       "  'Vehicles': ['# of bathrooms',\n",
       "   '# of bedrooms',\n",
       "   'Estimate',\n",
       "   'House-apartment-condo',\n",
       "   'Household ID',\n",
       "   'Square footage',\n",
       "   'Year built',\n",
       "   'nVehicles']},\n",
       " '_MAP_DATASETS_TO_DATA_TYPES_': {'People/# of riders': dtype('int64'),\n",
       "  'People/Cost': dtype('int64'),\n",
       "  'People/Gas-electric-human powered': dtype('O'),\n",
       "  'People/Household ID': dtype('int64'),\n",
       "  'People/Type of vehicle': dtype('O'),\n",
       "  'People/Year': dtype('int64'),\n",
       "  'People/nPeople': dtype('int64'),\n",
       "  'Residences/Age': dtype('int64'),\n",
       "  'Residences/First name': dtype('O'),\n",
       "  'Residences/Gender ID': dtype('O'),\n",
       "  'Residences/Height': dtype('int64'),\n",
       "  'Residences/Highest degree-grade': dtype('O'),\n",
       "  'Residences/Household ID': dtype('int64'),\n",
       "  'Residences/Last name': dtype('O'),\n",
       "  'Residences/Yearly income': dtype('int64'),\n",
       "  'Residences/nResidences': dtype('int64'),\n",
       "  'Vehicles/# of bathrooms': dtype('float64'),\n",
       "  'Vehicles/# of bedrooms': dtype('int64'),\n",
       "  'Vehicles/Estimate': dtype('int64'),\n",
       "  'Vehicles/House-apartment-condo': dtype('O'),\n",
       "  'Vehicles/Household ID': dtype('int64'),\n",
       "  'Vehicles/Square footage': dtype('int64'),\n",
       "  'Vehicles/Year built': dtype('int64'),\n",
       "  'Vehicles/nVehicles': dtype('int64'),\n",
       "  '_SINGLETONS_GROUP_': dtype('float64'),\n",
       "  '_SINGLETONS_GROUP_/COUNTER': dtype('float32')}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcaa42f-82d5-4b7c-ac85-ac0b309ce19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
