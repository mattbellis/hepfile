{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccd693-e497-4413-84d1-17bcfb037fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hepfile as hf\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "sys.path.insert(0, os.path.abspath(\"../src/hepfile\"))\n",
    "\n",
    "from src.hepfile.constants import *\n",
    "from errors import *\n",
    "from src.hepfile.read import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c439e3c-c76c-4468-b32e-03b30725aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_subset(packed_args) -> dict:\n",
    "    \n",
    "    '''\n",
    "    Reads a subset of the data, from the HDF5 file to fill a data dictionary.\n",
    "\n",
    "    Args:\n",
    "\tpacked_args (list): list with 1 x 4 dimensions in the order: infile, subset, verbose, desired_groups. \n",
    "                        subset, verbose, and desired_groups should be the same for all rows\n",
    "    \n",
    "    Returns:\n",
    "\tdata (dict): Selected data from HDF5 file\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # unpack args    \n",
    "    infile, subset, verbose, desired_groups = zip(*packed_args)\n",
    "    \n",
    "    # Create the initial data and bucket dictionary to hold the data\n",
    "    data = {}\n",
    "    bucket = {}\n",
    "\n",
    "    # We'll fill the data dictionary with some extra fields, though we won't\n",
    "    # need them all for the bucket\n",
    "    data[\"_MAP_DATASETS_TO_COUNTERS_\"] = {}\n",
    "    data[\"_MAP_DATASETS_TO_INDEX_\"] = {}\n",
    "    data[\"_LIST_OF_COUNTERS_\"] = []\n",
    "    data[\"_LIST_OF_DATASETS_\"] = []\n",
    "    data[\"_META_\"] = {}\n",
    "\n",
    "    # Get the number of buckets.\n",
    "    # In HEP (High Energy Physics), this would be the number of events\n",
    "    data[\"_NUMBER_OF_BUCKETS_\"] = infile.attrs[\"_NUMBER_OF_BUCKETS_\"]\n",
    "\n",
    "    if type(subset) is tuple:\n",
    "        subset = list(subset)\n",
    "\n",
    "    if type(subset) is int:\n",
    "        print(\"Single subset value of {subset} being interpreted as a high range\")\n",
    "        print(f\"subset being set to a range of (0,{subset})\\n\")\n",
    "        subset = [0, subset]\n",
    "\n",
    "    # If the user has specified `subset` incorrectly, then let's return\n",
    "    # an empty data and bucket\n",
    "    if subset[1]-subset[0]<=0:\n",
    "        raise RangeSubsetError(f\"The range in subset is either 0 or negative! {subset[1]} - {subset[0]} = {subset[1] - subset[0]}\")\n",
    "\n",
    "    # Make sure the user is not asking for something bigger than the file!\n",
    "    nbuckets = data[\"_NUMBER_OF_BUCKETS_\"]\n",
    "\n",
    "    if subset[0] > nbuckets:\n",
    "        raise RangeSubsetError(f'Range for subset starts greater than number of buckets in file! {subset[0]} > {nbuckets}')\n",
    "\n",
    "    if subset[1] > nbuckets:\n",
    "        warnings.warn(f'Range for subset is greater than number of buckets in file!\\n{subset[1]} > {nbuckets}\\nHigh range of subset will be set to {nbuckets}\\n')\n",
    "        subset[1] = nbuckets\n",
    "\n",
    "    data[\"_NUMBER_OF_BUCKETS_\"] = subset[1] - subset[0]\n",
    "\n",
    "    nbuckets = data[\"_NUMBER_OF_BUCKETS_\"]\n",
    "\n",
    "    ############################################################################\n",
    "    # Get the datasets and counters\n",
    "    ############################################################################\n",
    "    dc = infile[\"_MAP_DATASETS_TO_COUNTERS_\"]\n",
    "    for vals in dc:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Map datasets to counters: {vals}\")\n",
    "\n",
    "        # The decode is there because vals were stored as numpy.bytes\n",
    "        counter = vals[1].decode()\n",
    "        index = f\"{counter}_INDEX\"\n",
    "        data[\"_MAP_DATASETS_TO_COUNTERS_\"][vals[0].decode()] = counter\n",
    "        data[\"_MAP_DATASETS_TO_INDEX_\"][vals[0].decode()] = index\n",
    "        data[\"_LIST_OF_COUNTERS_\"].append(vals[1].decode())\n",
    "        data[\"_LIST_OF_DATASETS_\"].append(vals[0].decode())\n",
    "        data[\"_LIST_OF_DATASETS_\"].append(vals[1].decode())  # Get the counters as well\n",
    "\n",
    "    # We may have added some counters and datasets multiple times.\n",
    "    # So just to be sure, only keep the unique values\n",
    "    data[\"_LIST_OF_COUNTERS_\"] = np.unique(data[\"_LIST_OF_COUNTERS_\"]).tolist()\n",
    "    data[\"_LIST_OF_DATASETS_\"] = np.unique(data[\"_LIST_OF_DATASETS_\"]).tolist()\n",
    "    ############################################################################            \n",
    "\n",
    "    ############################################################################\n",
    "    # Pull out the SINGLETON datasets\n",
    "    ############################################################################\n",
    "    sg = infile[\"_SINGLETONSGROUPFORSTORAGE_\"][0]  # This is a numpy array of strings\n",
    "    decoded_string = sg[1].decode()\n",
    "\n",
    "    vals = decoded_string.split(\"__:__\")\n",
    "    vals.remove(\"COUNTER\")\n",
    "\n",
    "    data[\"_SINGLETONS_GROUP_\"] = vals\n",
    "    ############################################################################\n",
    "\n",
    "    ############################################################################\n",
    "    # Get the list of datasets and groups\n",
    "    ############################################################################\n",
    "    all_datasets = data[\"_LIST_OF_DATASETS_\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"all_datasets: {all_datasets}\")\n",
    "    ############################################################################\n",
    "\n",
    "    ############################################################################\n",
    "    # Only keep select data from file, if we have specified desired_groups\n",
    "    ############################################################################\n",
    "    if desired_groups is not None:\n",
    "        if type(desired_groups) != list:\n",
    "            desired_groups = list(desired_groups)\n",
    "\n",
    "        # Count backwards because we'll be removing stuff as we go.\n",
    "        i = len(all_datasets) - 1\n",
    "        while i >= 0:\n",
    "            entry = all_datasets[i]\n",
    "\n",
    "            is_dropped = True\n",
    "            # This is looking to see if the string is anywhere in the name\n",
    "            # of the dataset\n",
    "            for desdat in desired_groups:\n",
    "                if desdat in entry:\n",
    "                    is_dropped = False\n",
    "                    break\n",
    "\n",
    "            if is_dropped == True:\n",
    "                print(f\"Not reading out {entry} from the file....\")\n",
    "                all_datasets.remove(entry)\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"After only selecting certain datasets ----- \")\n",
    "            print(f\"all_datasets: {all_datasets}\")\n",
    "    ###########################################################################\n",
    "\n",
    "    # We might need the counter for SINGLETONS so let's pull it out\n",
    "    data[\"_SINGLETONS_GROUP_/COUNTER\"] = infile[\"_SINGLETONS_GROUP_\"]['COUNTER']\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\nDatasets and counters:\")\n",
    "        print(data[\"_MAP_DATASETS_TO_COUNTERS_\"])\n",
    "        print(\"\\nList of counters:\")\n",
    "        print(data[\"_LIST_OF_COUNTERS_\"])\n",
    "        print(\"\\n_SINGLETONS_GROUP_/COUNTER:\")\n",
    "        print(data[\"_SINGLETONS_GROUP_/COUNTER\"])\n",
    "        print(\"\\n\")\n",
    "\n",
    "    ############################################################################\n",
    "    # Pull out the counters and build the indices\n",
    "    ############################################################################\n",
    "    print(\"Building the indices...\\n\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"data.keys()\")\n",
    "        print(data.keys())\n",
    "        print(\"\\n\")\n",
    "\n",
    "    # We will need to keep track of the indices in the entire file\n",
    "    # This way, if the user specifies a subset of the data, we have the full \n",
    "    # indices already calculated\n",
    "    full_file_indices = {}\n",
    "\n",
    "    for counter_name in data[\"_LIST_OF_COUNTERS_\"]:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"counter name: ------------ {counter_name}\\n\")\n",
    "\n",
    "        full_file_counters = infile[counter_name]\n",
    "        full_file_index = calculate_index_from_counters(full_file_counters)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"full file counters: {full_file_counters}\\n\")\n",
    "            print(f\"full file index: {full_file_index}\\n\")\n",
    "\n",
    "        # If we passed in subset, grab that slice of the data from the file\n",
    "        if subset is not None and subset[1] <= subset[0]:\n",
    "            raise RangeSubsetError(f\"Unable to read anything in! High range of {subset[1]} is less than or equal to low range of {subset[0]}\")\n",
    "\n",
    "        elif subset is not None:\n",
    "            # We tack on +1 to the high range of subset when we pull out the counters\n",
    "            # and index because we want to get all of the entries for the last entry.\n",
    "            data[counter_name] = infile[counter_name][subset[0] : subset[1]+1]\n",
    "            index = full_file_index[subset[0] : subset[1]+1]\n",
    "        else:\n",
    "            data[counter_name] = infile[counter_name][:]\n",
    "            index = full_file_index\n",
    "\n",
    "        subset_index = index\n",
    "        # If the file is *not* empty....\n",
    "        # Just to make sure the \"local\" index of the data dictionary starts at 0\n",
    "        if len(index)>0:\n",
    "            subset_index = index - index[0]\n",
    "\n",
    "        index_name = \"%s_INDEX\" % (counter_name)\n",
    "\n",
    "        data[index_name] = subset_index\n",
    "        full_file_indices[index_name] = index\n",
    "\n",
    "    print(\"Built the indices!\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"full_file_index: \")\n",
    "        print(f\"{full_file_indices}\\n\")\n",
    "\n",
    "    # Loop over the all_datasets we want and pull out the data.\n",
    "    for name in all_datasets:\n",
    "\n",
    "        # If this is a counter, we're going to have to grab the indices\n",
    "        # differently than for a \"normal\" dataset\n",
    "        IS_COUNTER = True\n",
    "        index_name = None\n",
    "        if name not in data[\"_LIST_OF_COUNTERS_\"]:\n",
    "            index_name = data[\"_MAP_DATASETS_TO_INDEX_\"][name]\n",
    "            IS_COUNTER = False # We will use different indices for the counters\n",
    "\n",
    "        if verbose == True:\n",
    "            print(f\"------ {name}\")\n",
    "            print(f\"index_name: {index_name}\\n\")\n",
    "\n",
    "        dataset = infile[name]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"dataset type: {type(dataset)}\")\n",
    "\n",
    "        # This will ignore the groups\n",
    "        if type(dataset) == h5.Dataset:\n",
    "            dataset_name = name\n",
    "\n",
    "            if subset is not None:\n",
    "                if IS_COUNTER:\n",
    "                    # If this is a counter, then the subset indices\n",
    "                    # map on to the same locations for any counters\n",
    "                    lo = subset[0]\n",
    "                    hi = subset[1]\n",
    "                else:\n",
    "                    lo = full_file_indices[index_name][0]\n",
    "                    hi = full_file_indices[index_name][-1]\n",
    "                if verbose:\n",
    "                    print(f\"dataset name/lo/hi: {dataset_name},{lo},{hi}\\n\")\n",
    "                data[dataset_name] = dataset[lo : hi]\n",
    "            else:\n",
    "                data[dataset_name] = dataset[:]\n",
    "\n",
    "            bucket[dataset_name] = None  # This will be filled for individual bucket\n",
    "            if verbose == True:\n",
    "                print(dataset)\n",
    "\n",
    "        # write the metadata for that group to data if it exists\n",
    "        if name not in constants.protected_names and 'meta' in dataset.attrs.keys():\n",
    "            data['_META_'][name] = dataset.attrs['meta']\n",
    "                        \n",
    "    print(\"Data is read in and input file is closed.\")\n",
    "\n",
    "    # edit data so it matches the format of the data dict that was saved to the file\n",
    "    # this makes it so that data can be directly passed to write_to_file\n",
    "    # 1) add back in _GROUP_\n",
    "    datasets = np.array(data['_LIST_OF_DATASETS_'])\n",
    "\n",
    "    allgroups = np.array([d.split('/')[0] for d in datasets])\n",
    "    \n",
    "    singletons_group = set(data['_SINGLETONS_GROUP_'])\n",
    "    groups = {}\n",
    "    \n",
    "    groups['_SINGLETONS_GROUP_'] = data['_SINGLETONS_GROUP_'] # copy over the data\n",
    "    \n",
    "    for key in np.unique(allgroups):\n",
    "\n",
    "        if key in singletons_group: continue\n",
    "        if key in constants.protected_names: continue\n",
    "        \n",
    "        where_groups = np.where((key == allgroups) * (key != datasets))[0]\n",
    "        groups[key] = [dataset.split('/')[-1] for dataset in datasets[where_groups]]\n",
    "        \n",
    "    data['_GROUPS_'] = groups\n",
    "\n",
    "    # 2) add back in _MAP_DATASETS_TO_DATA_TYPES\n",
    "    dtypes = {}\n",
    "    for key in data['_LIST_OF_DATASETS_']:\n",
    "\n",
    "        if key not in data.keys():\n",
    "            continue\n",
    "        \n",
    "        if isinstance(data[key], list):\n",
    "            data[key] = np.array(data[key])\n",
    "\n",
    "        dtypes[key] = data[key].dtype\n",
    "        \n",
    "    data['_MAP_DATASETS_TO_DATA_TYPES_'] = dtypes\n",
    "\n",
    "    return data, bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb8fa4-95b6-493d-b32e-c7ade644d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename:str, num_cores:int=mp.cpu_count(), verbose:bool=False, desired_groups:list[str]=None, subset:int=None, return_awkward:bool=False) -> tuple[dict, dict]:\n",
    "    '''\n",
    "    Reads all, or a subset of the data, from the HDF5 file to fill a data dictionary.\n",
    "    Returns an empty dictionary to be filled later with data from individual buckets.\n",
    "\n",
    "    Args:\n",
    "\tfilename (string): Name of the input file\n",
    "\t\n",
    "\tverbose (boolean): True if debug output is required\n",
    "\n",
    "\tdesired_groups (list): Groups to be read from input file, \n",
    "\n",
    "\tsubset (int): Number of buckets to be read from input file\n",
    "\n",
    "        return_awkward (boolean): If True, returns an awkward array Record. Default is False\n",
    "\n",
    "    Returns:\n",
    "\tdata (dict): Selected data from HDF5 file\n",
    "\t\n",
    "\tbucket (dict): An empty bucket dictionary to be filled by data from select buckets\n",
    "\n",
    "    '''\n",
    "    \n",
    "    with h5.File(filename, 'r+') as infile:\n",
    "        \n",
    "        nbuckets = infile.attrs[\"_NUMBER_OF_BUCKETS_\"]\n",
    "        if subset is None:\n",
    "            subset = (0, nbuckets)\n",
    "        \n",
    "        # check number of cores\n",
    "        if num_cores > nbuckets:\n",
    "            warnings.warn('num_cores is greater than nbuckets, reducing number of cores used to the number of buckets!')\n",
    "            num_cores = nbuckets\n",
    "        \n",
    "        # pack up the arguments to pass to multiprocessing pool\n",
    "        per_core = np.ceil((subset[1] - subset[0]) / num_cores)\n",
    "        all_subsets = []\n",
    "        min_val = subset[0]\n",
    "        max_val = -1\n",
    "        while max_val < subset[1]:\n",
    "            max_val = min_val + per_core\n",
    "            if max_val > nbuckets:\n",
    "                max_val = nbuckets\n",
    "            all_subsets.append((int(min_val), int(max_val)))\n",
    "            min_val += per_core\n",
    "        \n",
    "        n = len(all_subsets)\n",
    "        packed_args = list(zip([infile]*n, all_subsets, [verbose]*n, [desired_groups]*n))\n",
    "        \n",
    "        with mp.Pool(num_cores) as p:\n",
    "            out = p.apply(_load_subset, packed_args)\n",
    "        \n",
    "        #data, bucket = _load_subset(infile, verbose=verbose, desired_groups=desired_groups, subset=subset)\n",
    "        print(out)\n",
    "#     if return_awkward:\n",
    "#         from hepfile.awkward_tools import hepfile_to_awkward\n",
    "#         return hepfile_to_awkward(data), bucket\n",
    "    \n",
    "#     return data, bucket\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fcb57-22e8-4905-b94f-6e769a90eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.abspath('../docs/example_nb/updated-awkward-array.h5')\n",
    "data, bucket = load(filepath, num_cores=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070e494-271c-474d-adb5-1240aa7ea4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcaa42f-82d5-4b7c-ac85-ac0b309ce19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
