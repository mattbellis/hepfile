{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833f7a54-de23-42c3-9539-7f867fbfaf18",
   "metadata": {},
   "source": [
    "# Writing a hepfile from Scratch\n",
    "\n",
    "The method shown in this tutorial is what is done under the hood in the `hepfile.dict_tools.dictlike_to_hepfile` method. This may be preferable for those users who want more control over the structure of their hepfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2fb291-9793-4f9c-a12d-84e60fe7e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from hepfile import write as writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220bb75-f076-4066-a201-d73c81af7389",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once hepfile.write is imported, we need to start by initializing the empty data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21fffe4e-de19-4a1d-9b2c-d4da9b49b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = writer.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515cf60c-9fe5-430d-ab8a-1503ede01b21",
   "metadata": {},
   "source": [
    "Now that the empty data structure is initialized, we must set up groups and datasets. Groups hold datasets and datasets hold data about that group. You can also just create higher level datasets that are considered \"singletons\" which do not correspond to a specific group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea1fd13-ce7a-44f8-a6ac-6e49b895e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the groups\n",
    "writer.create_group(data,'jet',counter='njet')\n",
    "writer.create_group(data,'muons',counter='nmuon')\n",
    "\n",
    "# add datasets to different groups\n",
    "writer.create_dataset(data,['e','px','py','pz'],group='jet',dtype=float)\n",
    "writer.create_dataset(data,['algorithm'],group='jet',dtype=int)\n",
    "writer.create_dataset(data,['words'],group='jet',dtype=str)\n",
    "writer.create_dataset(data,['e','px','py','pz'],group='muons',dtype=float)\n",
    "\n",
    "# add a higher level dataset that doesn't have a group, a \"singleton\"\n",
    "writer.create_dataset(data,['METpx','METpy'],dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90305e-33d3-42c6-bad8-76712703f63b",
   "metadata": {},
   "source": [
    "To start adding data to the groups and datasets we have to generate an empty bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a24843-e480-4df2-a4d6-e027db36bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = writer.create_single_bucket(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457380e2-d7f8-43bd-bb35-c367c8781636",
   "metadata": {},
   "source": [
    "Now that we have the hepfile structure set up, we can generate random data and insert it into the hepfile. This is done by appending data to the current dataset lists that were created above in a \"bucket\" and then packing this \"bucket\" into the data dictionary. Note that packing is much more efficient if the bucket and data dictionaries are made up of python lists instead of NumPy Arrays but both data structures work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed60a79-aebc-494b-b78d-c351c2b39a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rando_words = [\"hi\", \"bye\", \"ciao\", \"aloha\"]\n",
    "\n",
    "for i in range(0,1000):\n",
    "\n",
    "    #hepfile.clear_event(event)\n",
    "\n",
    "    njet = 17\n",
    "    bucket['jet/njet'] = njet\n",
    "\n",
    "    for n in range(njet):\n",
    "        bucket['jet/e'].append(np.random.random())\n",
    "        bucket['jet/px'].append(np.random.random())\n",
    "        bucket['jet/py'].append(np.random.random())\n",
    "        bucket['jet/pz'].append(np.random.random())\n",
    "\n",
    "        bucket['jet/algorithm'].append(np.random.randint(-1,1))\n",
    "\n",
    "        bucket['jet/words'].append(np.random.choice(rando_words))\n",
    "\n",
    "    bucket['METpx'] = np.random.random()\n",
    "    bucket['METpy'] = np.random.random()\n",
    "\n",
    "    #hepfile.pack(data,event,EMPTY_OUT_BUCKET=False)\n",
    "    return_value = writer.pack(data,bucket,STRICT_CHECKING=True)\n",
    "    if return_value != 0:\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e08af-966d-4420-af65-6a5a0619cb0d",
   "metadata": {},
   "source": [
    "Finally, we are ready to write this random data to a file called `output.hdf5`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ab1dbd-054c-4628-8939-65227f52dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfile = writer.write_to_file('output_from_scratch.hdf5',data,comp_type='gzip',comp_opts=9,verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
