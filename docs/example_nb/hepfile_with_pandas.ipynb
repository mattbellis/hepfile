{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75271386-afe8-4ff8-a786-90186ac89ba7",
   "metadata": {},
   "source": [
    "# Working with Pandas DataFrames\n",
    "\n",
    "We can also convert from hepfile's dictionary structure to a dictionary of pandas dataframes that are organized into the groups that were in the hepfile. This is possible because the datasets under each group are the same length even though the datasets across groups are not necessarily the same length.\n",
    "\n",
    "First, make sure that you have installed hepfile using one of the following commands. The base installation does not have these pandas tools built in!\n",
    "1. `python -m pip install hepfile[all]`, or\n",
    "2. `python -m pip install hepfile[pandas]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efc2cb5-b82f-4c5d-afeb-439f3873479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hepfile as hf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e003e9-1fa5-4904-9268-09bf3a7c7b24",
   "metadata": {},
   "source": [
    "## Hepfiles from Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1868c9-5dc7-417d-a617-58067fbe8c7a",
   "metadata": {},
   "source": [
    "Let's create a hepfile from a dictionary of pandas dataframes. The key here is that the dictionary must have keys of what you want the group names to be in the hepfile and each Pandas DataFrame in the dictionary has columns with dataset names. All of the singletons should be stored in a table called '_SINGLETONS_GROUP_' so that they are stored properly. Finally, we also need to add a column with the event number of each row so that we can keep it all straight.\n",
    "\n",
    "Here are the three groups/singletons we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7b216d-1318-4aed-9228-c647e7aa507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = pd.DataFrame({\n",
    "    'x': [1, 2, 3, 4],\n",
    "    'y': [5, 6, 7, 8],\n",
    "    'event_num': [0, 0, 1, 1]\n",
    "})\n",
    "\n",
    "group2 = pd.DataFrame({\n",
    "    'z': [10.0, 11.0, 12.5],\n",
    "    'w': [1600, 25, 16],\n",
    "    'event_num': [0, 0, 1]\n",
    "})\n",
    "\n",
    "singletons = pd.DataFrame({\n",
    "    'some_singleton': [1, 2],\n",
    "    'event_num': [0, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e62b05-dc8f-4b84-9061-929bddef7a4e",
   "metadata": {},
   "source": [
    "Now we pack these into a single dictionary where the keys are the group names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee4f35e-a944-40e3-b7c6-e18e88c93dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group_name_1':    x  y  event_num\n",
       " 0  1  5          0\n",
       " 1  2  6          0\n",
       " 2  3  7          1\n",
       " 3  4  8          1,\n",
       " 'group_name_2':       z     w  event_num\n",
       " 0  10.0  1600          0\n",
       " 1  11.0    25          0\n",
       " 2  12.5    16          1,\n",
       " '_SINGLETONS_GROUP_':    some_singleton  event_num\n",
       " 0               1          0\n",
       " 1               2          1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " indata = {\n",
    "     'group_name_1': group1,\n",
    "     'group_name_2': group2,\n",
    "     '_SINGLETONS_GROUP_': singletons\n",
    " }\n",
    "    \n",
    "indata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c234201-d110-4614-b821-d568d8c5947f",
   "metadata": {},
   "source": [
    "Once we have this data, we can pass it into `hf.df_tools.df_to_hepfile` to convert it to a hepfile. But first let's check out the options for this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58999b10-e5da-4703-b538-41eb8769a1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function df_to_hepfile in module hepfile.df_tools:\n",
      "\n",
      "df_to_hepfile(df_dict: 'dict[pd.DataFrame]', outfile: 'str' = None, event_num_col='event_num', write_hepfile: 'bool' = True) -> 'dict'\n",
      "    Converts a list of dataframes of group data to a hepfile. The opposite of\n",
      "    hepfile_to_df. Must have an event_num column!\n",
      "    \n",
      "    Args:\n",
      "        df_dict (dict): dictionary of pandas DataFrame groups to write to a hepfile\n",
      "        outfile (str): output file name, required if write_hepfile is True\n",
      "        event_num_col (str): name of a column in the pd.DataFrame to group by\n",
      "        write_hepfile (bool): should we write the hepfile data to a hepfile?\n",
      "    \n",
      "    Returns:\n",
      "        dict: hepfile data dictionary\n",
      "    \n",
      "    Raises:\n",
      "        InputError: If something is wrong with the specific input.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hf.df_tools.df_to_hepfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060c87c-5b0b-441b-ae13-1e383a5b8494",
   "metadata": {},
   "source": [
    "So this function takes in the dictionary of pandas dataframes that we created. The other optional inputs are the outfile path, which is necessary if we want to write the hepfile data to a hepfile, the write_hepfile boolean which just says whether we want to write to a file, and the event_num_col which is defaulted to 'event_num' but can be changed to a different column name too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e76783-384a-4296-9e17-9789706468e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_GROUPS_': {'_SINGLETONS_GROUP_': ['COUNTER', 'event_num', 'some_singleton'],\n",
       "  'group_name_1': ['ngroup_name_1', 'x', 'y'],\n",
       "  'group_name_2': ['ngroup_name_2', 'z', 'w']},\n",
       " '_MAP_DATASETS_TO_COUNTERS_': {'_SINGLETONS_GROUP_': '_SINGLETONS_GROUP_/COUNTER',\n",
       "  'group_name_1': 'group_name_1/ngroup_name_1',\n",
       "  'group_name_1/x': 'group_name_1/ngroup_name_1',\n",
       "  'group_name_1/y': 'group_name_1/ngroup_name_1',\n",
       "  'event_num': '_SINGLETONS_GROUP_/COUNTER',\n",
       "  'group_name_2': 'group_name_2/ngroup_name_2',\n",
       "  'group_name_2/z': 'group_name_2/ngroup_name_2',\n",
       "  'group_name_2/w': 'group_name_2/ngroup_name_2',\n",
       "  'some_singleton': '_SINGLETONS_GROUP_/COUNTER'},\n",
       " '_LIST_OF_COUNTERS_': ['_SINGLETONS_GROUP_/COUNTER',\n",
       "  'group_name_1/ngroup_name_1',\n",
       "  'group_name_2/ngroup_name_2'],\n",
       " '_SINGLETONS_GROUP_/COUNTER': [1, 1],\n",
       " '_MAP_DATASETS_TO_DATA_TYPES_': {'_SINGLETONS_GROUP_/COUNTER': int,\n",
       "  'group_name_1/ngroup_name_1': int,\n",
       "  'group_name_1/x': numpy.int64,\n",
       "  'group_name_1/y': numpy.int64,\n",
       "  'event_num': numpy.int64,\n",
       "  'group_name_2/ngroup_name_2': int,\n",
       "  'group_name_2/z': numpy.float64,\n",
       "  'group_name_2/w': numpy.int64,\n",
       "  'some_singleton': numpy.int64},\n",
       " '_META_': {},\n",
       " 'group_name_1/ngroup_name_1': [2, 2],\n",
       " 'group_name_1/x': [1, 2, 3, 4],\n",
       " 'group_name_1/y': [5, 6, 7, 8],\n",
       " 'event_num': [0, 1],\n",
       " 'group_name_2/ngroup_name_2': [2, 1],\n",
       " 'group_name_2/z': [10.0, 11.0, 12.5],\n",
       " 'group_name_2/w': [1600, 25, 16],\n",
       " 'some_singleton': [1, 2]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = hf.df_tools.df_to_hepfile(indata, event_num_col='event_num', write_hepfile=True, outfile='pandas_test.h5')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed58dd9-fb28-4da8-9164-a6cc9975baa2",
   "metadata": {},
   "source": [
    "## Pandas DataFrames from hepfiles\n",
    "\n",
    "Now that we have this data stored in a hepfile, we can load it into a Pandas DataFrame in two ways. First, the more restrictive (but easier) option is to pass `return_type='pandas'` to `hf.load`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd1f19c7-5509-4c12-b5e5-a3f4d2ff0b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_SINGLETONS_GROUP_':    event_num  some_singleton\n",
       " 0          0               1\n",
       " 1          1               2,\n",
       " 'group_name_1':    x  y  event_num\n",
       " 0  1  5          0\n",
       " 1  2  6          0\n",
       " 2  3  7          1\n",
       " 3  4  8          1,\n",
       " 'group_name_2':       w     z  event_num\n",
       " 0  1600  10.0          0\n",
       " 1    25  11.0          0\n",
       " 2    16  12.5          1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs, bucket = hf.load('pandas_test.h5', return_type='pandas')\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2511f-01c2-4bea-b9d8-1ce4c43a778c",
   "metadata": {},
   "source": [
    "As you can see, this easily loads the hepfile into the same dictionary of dataframes that we wrote to it! But, there are two limitations to this method:\n",
    "1. We can't select a subset of the data (like a subset of groups or subset of event numbers) from the hepfile\n",
    "2. The event number column must be called `event_num`\n",
    "\n",
    "So, to fix this, we can also load the hepfile into a hepfile dictionary object and then convert it using the `hf.df_tools.hepfile_to_df` function. This function takes in a hepfile dictionary and optionally a list (or string) of group names and a list (or int) of event numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd1e32e-e908-4ad9-8534-1e8675494aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_SINGLETONS_GROUP_':    event_num  some_singleton\n",
       " 0          0               1\n",
       " 1          1               2,\n",
       " 'group_name_1':    x  y  event_num\n",
       " 0  1  5          0\n",
       " 1  2  6          0\n",
       " 2  3  7          1\n",
       " 3  4  8          1,\n",
       " 'group_name_2':       w     z  event_num\n",
       " 0  1600  10.0          0\n",
       " 1    25  11.0          0\n",
       " 2    16  12.5          1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, bucket = hf.load('pandas_test.h5', return_type='dictionary')\n",
    "dfs = hf.df_tools.hepfile_to_df(data) # this loads all the data\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3edd487-ae82-431d-8d79-26e7f94019a8",
   "metadata": {},
   "source": [
    "Now say that we only want group 1, we would use the `groups` option. Notice how it returns a dataframe directly if only one group is specified!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c8002f-b38b-4553-8721-a741792792c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x  y  event_num\n",
      "0  1  5          0\n",
      "1  2  6          0\n",
      "2  3  7          1\n",
      "3  4  8          1\n"
     ]
    }
   ],
   "source": [
    "df = hf.df_tools.hepfile_to_df(data, groups='group_name_1') # this loads just group 1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8929a0d-fc78-4dc7-9163-357d6e5c38e7",
   "metadata": {},
   "source": [
    "Finally, say we only want values associated with event 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f8eed7c-67b9-4d99-8489-33c2457e5108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x  y  event_num\n",
      "2  3  7          1\n",
      "3  4  8          1\n"
     ]
    }
   ],
   "source": [
    "df = hf.df_tools.hepfile_to_df(data, groups='group_name_1', events=1) # this loads just group 1 and event 1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863a7bb8-709b-4351-9055-8d2c4c3d157d",
   "metadata": {},
   "source": [
    "## Working with awkward arrays of hepfiles and pandas dataframes\n",
    "\n",
    "We can also convert awkward arrays of hepfile data to pandas dataframes. (Note: this basically just wraps on the official awkward array to dataframe method so for better performance and more options check that out!)\n",
    "\n",
    "Let's start by converting `data` from above to an awkward array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84150ff6-f0f1-426c-b1d0-8f512b1226f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{event_num: 0, some_singleton: 1, group_name_1: {...}, ...}, {...}]\n"
     ]
    }
   ],
   "source": [
    "awk = hf.awkward_tools.hepfile_to_awkward(data)\n",
    "print(awk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9772644-5e83-419a-bd54-6d8448228495",
   "metadata": {},
   "source": [
    "Now we can convert that awkward array to a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72bd6d0-84a1-4157-8cbe-e3021be076ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group_name_1':                 x  y  event_num\n",
      "entry subentry                 \n",
      "0     0         1  5          0\n",
      "      1         2  6          0\n",
      "1     0         3  7          1\n",
      "      1         4  8          1, 'group_name_2':                    w     z  event_num\n",
      "entry subentry                       \n",
      "0     0         1600  10.0          0\n",
      "      1           25  11.0          0\n",
      "1     0           16  12.5          1, '_SINGLETONS_GROUP_':    event_num  some_singleton\n",
      "0          0               1\n",
      "1          1               2}\n"
     ]
    }
   ],
   "source": [
    "dfs = hf.df_tools.awkward_to_df(awk)\n",
    "print(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea186d98-112e-4fb8-8ddf-fb2cdc4e5d41",
   "metadata": {},
   "source": [
    "It's that easy to navigate between the different data types! One important thing to note is that the dataframes returned by `awkward_to_df` have nested indexes while the dataframes returned by `hepfile_to_df` just have a single index. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
