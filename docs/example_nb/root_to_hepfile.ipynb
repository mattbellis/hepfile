{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93f1510-d4f5-41c9-9c97-a4ce8036d3e9",
   "metadata": {},
   "source": [
    "# Converting a ROOT File to a hepfile\n",
    "Since an important application of `hepfile` is storing high energy physics data, then many users may want to convert their ROOT files to\n",
    "hepfiles. This tutorial walks through how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd46c16-e8cd-4a9e-a720-2a8ebef0ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# note that you may need to pip install uproot\n",
    "import hepfile as hf\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef811f10-dc96-4c17-921b-5903e3fda5b1",
   "metadata": {},
   "source": [
    "First, we need to download a ROOT file from CERN's open data repository. This file is large so it may take some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922c7eff-8807-4ed7-beb5-d18082a444cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 40.4M  100 40.4M    0     0  11.1M      0  0:00:03  0:00:03 --:--:-- 11.1M\n"
     ]
    }
   ],
   "source": [
    "# Down load a file for us to play with\n",
    "!curl http://opendata.cern.ch/record/12361/files/SMHiggsToZZTo4L.root --output SMHiggsToZZTo4L.root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42223bb2-68f3-4de7-bdc8-e13f138bea5d",
   "metadata": {},
   "source": [
    "Now we can use `uproot` to read in the ROOT data and look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48f51e2-2ca5-4185-9fe4-3db88122b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events:\n",
      "<TTree 'Events' (32 branches) at 0x7f8d9838f1c0>\n",
      "\n",
      "Keys in Events:\n",
      "['run', 'luminosityBlock', 'event', 'PV_npvs', 'PV_x', 'PV_y', 'PV_z', 'nMuon', 'Muon_pt', 'Muon_eta', 'Muon_phi', 'Muon_mass', 'Muon_charge', 'Muon_pfRelIso03_all', 'Muon_pfRelIso04_all', 'Muon_dxy', 'Muon_dxyErr', 'Muon_dz', 'Muon_dzErr', 'nElectron', 'Electron_pt', 'Electron_eta', 'Electron_phi', 'Electron_mass', 'Electron_charge', 'Electron_pfRelIso03_all', 'Electron_dxy', 'Electron_dxyErr', 'Electron_dz', 'Electron_dzErr', 'MET_pt', 'MET_phi']\n"
     ]
    }
   ],
   "source": [
    "# This is all for demonstration purposes, to show people how this type of\n",
    "# writing could be done. \n",
    "# But of course people could just create their own awkward arrays.\n",
    "\n",
    "f = uproot.open('SMHiggsToZZTo4L.root')\n",
    "events = f['Events']\n",
    "print('Events:')\n",
    "print(events)\n",
    "print()\n",
    "print('Keys in Events:')\n",
    "print(events.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c6be7-be51-42ca-b901-4c691e63e47f",
   "metadata": {},
   "source": [
    "`uproot` reads in the ROOT file as a `TTree` object so we need to parse this into a form that is easier to work with. # While not all the entries in the ROOT file naturally lend themselves to group/dataset breakdowns, some do. Let's find those \"automatically\", just to make it easier to write them to the hepfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903d1ace-cdda-4cff-9fc2-bb448fb49101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PV': [['PV_npvs', 'npvs'], ['PV_x', 'x'], ['PV_y', 'y'], ['PV_z', 'z']], 'Muon': [['Muon_pt', 'pt'], ['Muon_eta', 'eta'], ['Muon_phi', 'phi'], ['Muon_mass', 'mass'], ['Muon_charge', 'charge'], ['Muon_pfRelIso03_all', 'pfRelIso03_all'], ['Muon_pfRelIso04_all', 'pfRelIso04_all'], ['Muon_dxy', 'dxy'], ['Muon_dxyErr', 'dxyErr'], ['Muon_dz', 'dz'], ['Muon_dzErr', 'dzErr']], 'Electron': [['Electron_pt', 'pt'], ['Electron_eta', 'eta'], ['Electron_phi', 'phi'], ['Electron_mass', 'mass'], ['Electron_charge', 'charge'], ['Electron_pfRelIso03_all', 'pfRelIso03_all'], ['Electron_dxy', 'dxy'], ['Electron_dxyErr', 'dxyErr'], ['Electron_dz', 'dz'], ['Electron_dzErr', 'dzErr']], 'MET': [['MET_pt', 'pt'], ['MET_phi', 'phi']]}\n",
      "\n",
      "[['Muon_pt', 'pt'], ['Muon_eta', 'eta'], ['Muon_phi', 'phi'], ['Muon_mass', 'mass'], ['Muon_charge', 'charge'], ['Muon_pfRelIso03_all', 'pfRelIso03_all'], ['Muon_pfRelIso04_all', 'pfRelIso04_all'], ['Muon_dxy', 'dxy'], ['Muon_dxyErr', 'dxyErr'], ['Muon_dz', 'dz'], ['Muon_dzErr', 'dzErr']]\n"
     ]
    }
   ],
   "source": [
    "# Find groups\n",
    "def make_groups_and_datasets(fields):\n",
    "    \n",
    "    groups = {}\n",
    "    \n",
    "    for field in fields:\n",
    "        if field.find('_')>=0:\n",
    "            \n",
    "            # Do this in case there is more than one underscore\n",
    "            idx = field.find('_')\n",
    "            \n",
    "            #print(field)\n",
    "            grp = field[0:idx]\n",
    "            dset = field[idx+1:]\n",
    "            \n",
    "            if grp not in groups.keys():\n",
    "                groups[grp] = [[field,dset]]\n",
    "            else:\n",
    "                groups[grp].append([field,dset])\n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "groupings = make_groups_and_datasets(events.keys())\n",
    "\n",
    "# Groupings gives us a nice mapping of the names from the ROOT file\n",
    "# to how we're going to store them in our hepfile as \n",
    "# group/datasets\n",
    "print(groupings)\n",
    "print()\n",
    "print(groupings['Muon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b177d-92a2-46e9-8ddf-569ec0d68f8e",
   "metadata": {},
   "source": [
    "The datasets that do not fit in this group/dataset structure can be written as singletons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1adc6d-9d71-460e-8914-9b5e027989a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..., 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[156, 156, 156, 156, 156, 156, 156, 156, ..., 996, 996, 996, 996, 996, 996, 996]\n",
      "[46501, 46502, 46503, 46504, 46505, ..., 298796, 298797, 298798, 298799, 298800]\n"
     ]
    }
   ],
   "source": [
    "# There are some others. THese will be SINGLETONS that we pass in separately.\n",
    "# 'run',\n",
    "# 'luminosityBlock',\n",
    "# 'event',\n",
    "\n",
    "print(events['run'].array())\n",
    "print(events['luminosityBlock'].array())\n",
    "print(events['event'].array())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc186cc-360d-4661-b47f-d140a6e44110",
   "metadata": {},
   "source": [
    "We can then write this data to a hepfile using `hf.awkward_tools.pack_multiple_awkward_arrays`. First, we need to initialize a data dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8b10b3-ee70-4829-94a7-6a2ff9576dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data dictionary\n",
    "data = hf.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a9451-d317-4e66-832a-c48e3ac906d1",
   "metadata": {},
   "source": [
    "Then, we pack the groups and dataset pairs into `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a22412a-4506-40f1-91c4-469ab4adeb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack these groups of awkward arrays\n",
    "\n",
    "# This is what it would look like \"by hand\"\n",
    "# A dictionary with the name of the dataset as it is to appear inside the hepfile\n",
    "# and then the actual awkward array (not just the Branch object returned by uproot)\n",
    "\n",
    "# Here I'm packing all the data that are groups/datasets\n",
    "for groups_to_write in ['Muon', 'Electron', 'MET', 'PV']:\n",
    "    ak_arrays = {}\n",
    "    for grouping in groupings[groups_to_write]:\n",
    "        ak_arrays[grouping[1]] = events[grouping[0]].array()\n",
    "    \n",
    "    hf.awkward_tools.pack_multiple_awkward_arrays(data, ak_arrays, group_name=groups_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5937b17-750c-40b5-a932-ee68466c4293",
   "metadata": {},
   "source": [
    "The, we can pack the singletons into the hepfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c526526-da78-4041-b215-1c2d29a1cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the SINGLETONS\n",
    "ak_arrays = {\"run\":events['run'].array(), \\\n",
    "             \"luminosityBlock\":events['luminosityBlock'].array(), \\\n",
    "             \"event\":events['event'].array()}\n",
    "\n",
    "# Note that there is no group name passed in. \n",
    "hf.awkward_tools.pack_multiple_awkward_arrays(data, ak_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a4535-0253-4712-874f-23867035d883",
   "metadata": {},
   "source": [
    "Let's take a look at the keys in `data` and see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68af993f-c9b1-4ba7-b25a-fc42d4ca338c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_GROUPS_', '_MAP_DATASETS_TO_COUNTERS_', '_LIST_OF_COUNTERS_', '_SINGLETONS_GROUP_/COUNTER', '_MAP_DATASETS_TO_DATA_TYPES_', '_META_', 'Muon/pt', 'Muon/nMuon', 'Muon/eta', 'Muon/phi', 'Muon/mass', 'Muon/charge', 'Muon/pfRelIso03_all', 'Muon/pfRelIso04_all', 'Muon/dxy', 'Muon/dxyErr', 'Muon/dz', 'Muon/dzErr', 'Electron/pt', 'Electron/nElectron', 'Electron/eta', 'Electron/phi', 'Electron/mass', 'Electron/charge', 'Electron/pfRelIso03_all', 'Electron/dxy', 'Electron/dxyErr', 'Electron/dz', 'Electron/dzErr', 'MET/pt', 'MET/nMET', 'MET/phi', 'PV/npvs', 'PV/nPV', 'PV/x', 'PV/y', 'PV/z', 'run', 'luminosityBlock', 'event'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8026a-c0be-41d9-b6ff-addce7a07762",
   "metadata": {},
   "source": [
    "It looks good! So, finally, we write this data to a hepfile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c96f455b-a34c-4b4e-947c-71debd6be20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Closed HDF5 file>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try it with no compression\n",
    "hf.write_to_file('root_to_hepfile.h5', data, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2b432-b366-4ae2-97e0-3d6c696c67fb",
   "metadata": {},
   "source": [
    "This is a rudimentary example and you can imagine making this process more automated, especially if you need to do this on lots of files. But, for now, this is an efficient way to convert a large ROOT file into a hepfile!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
